# ØªÙˆØ³Ø¹Ø© Ø§Ù„Ù†Ø¸Ø§Ù… - System Scaling

## Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©

ÙŠÙ‚Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙˆØ³Ø¹Ø© Ø´Ø§Ù…Ù„Ø© Ù„ØªØ·Ø¨ÙŠÙ‚ SalerØŒ Ù…Ù† Ø§Ù„ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø£ÙÙ‚ÙŠØ© ÙˆØ§Ù„Ø¹Ù…ÙˆØ¯ÙŠØ© Ø¥Ù„Ù‰ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­Ù…Ù„ ÙˆØ§Ù„ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø®ÙˆØ§Ø¯Ù…ØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹ ÙˆØ§Ù„Ù†Ù…Ùˆ Ù…Ø¹ Ø§Ø²Ø¯ÙŠØ§Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

## Ù…Ø­ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ù„ÙŠÙ„

1. [ØªØµÙ…ÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹](#ØªØµÙ…ÙŠÙ…-Ø§Ù„Ù†Ø¸Ø§Ù…-Ø§Ù„Ù‚Ø§Ø¨Ù„-Ù„Ù„ØªÙˆØ³Ø¹)
2. [Ø§Ù„ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠØ© (Vertical Scaling)](#Ø§Ù„ØªÙˆØ³Ø¹Ø©-Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠØ©-vertical-scaling)
3. [Ø§Ù„ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø£ÙÙ‚ÙŠØ© (Horizontal Scaling)](#Ø§Ù„ØªÙˆØ³Ø¹Ø©-Ø§Ù„Ø£ÙÙ‚ÙŠØ©-horizontal-scaling)
4. [ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø­Ù…Ø§Ù„ (Load Balancing)](#ØªÙˆØ²ÙŠØ¹-Ø§Ù„Ø£Ø­Ù…Ø§Ù„-load-balancing)
5. [Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙˆØ³Ø¹](#Ù‚Ø§Ø¹Ø¯Ø©-Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª-Ø§Ù„Ù‚Ø§Ø¨Ù„Ø©-Ù„Ù„ØªÙˆØ³Ø¹)
6. [Ø§Ù„ØªØ®Ø²ÙŠÙ† ÙˆØ§Ù„ØªÙˆØ²ÙŠØ¹](#Ø§Ù„ØªØ®Ø²ÙŠÙ†-ÙˆØ§Ù„ØªÙˆØ²ÙŠØ¹)
7. [Caching Ø§Ù„Ù…ØªÙ‚Ø¯Ù…](#caching-Ø§Ù„Ù…ØªÙ‚Ø¯Ù…)
8. [Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©](#Ø¥Ø¯Ø§Ø±Ø©-Ø§Ù„Ø¨Ù†ÙŠØ©-Ø§Ù„ØªØ­ØªÙŠØ©)
9. [Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„ØªÙˆØ³Ø¹Ø©](#Ù…Ø±Ø§Ù‚Ø¨Ø©-Ø§Ù„Ø£Ø¯Ø§Ø¡-ÙˆØ§Ù„ØªÙˆØ³Ø¹Ø©)
10. [Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªÙˆØ³Ø¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠØ©](#Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª-Ø§Ù„ØªÙˆØ³Ø¹Ø©-Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠØ©)

## ØªØµÙ…ÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹

### Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹

```mermaid
graph TB
    subgraph "Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¹Ø±Ø¶"
        A[CDN/Cloudflare] --> B[Load Balancer]
        B --> C[Web Servers]
    end
    
    subgraph "Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚"
        C --> D[API Gateway]
        D --> E[Application Servers]
        E --> F[Message Queue]
    end
    
    subgraph "Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"
        F --> G[Database Primary]
        F --> H[Database Replicas]
        G --> I[Redis Cluster]
        H --> I
    end
    
    subgraph "Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†"
        I --> J[Object Storage]
        J --> K[File Storage]
    end
    
    subgraph "Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©"
        L[Monitoring] --> M[Metrics]
        L --> N[Logs]
        L --> O[Alerts]
    end
    
    style A fill:#e1f5fe
    style G fill:#f3e5f5
    style I fill:#e8f5e8
    style L fill:#fff3e0
```

### Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„ØªØµÙ…ÙŠÙ… Ø§Ù„Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹

```python
# scalable_design.py - Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„ØªØµÙ…ÙŠÙ… Ø§Ù„Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from enum import Enum
import asyncio
import logging

logger = logging.getLogger(__name__)

class ScalingStrategy(Enum):
    """Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªÙˆØ³Ø¹Ø©"""
    VERTICAL = "vertical"
    HORIZONTAL = "horizontal"
    AUTO_SCALING = "auto_scaling"
    HYBRID = "hybrid"

class ServiceType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø®Ø¯Ù…Ø§Øª"""
    WEB_SERVER = "web_server"
    API_SERVER = "api_server"
    DATABASE = "database"
    CACHE = "cache"
    QUEUE = "queue"
    STORAGE = "storage"

class ScalableComponent(ABC):
    """Ù…ÙƒÙˆÙ† Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹"""
    
    def __init__(self, service_type: ServiceType, config: Dict[str, Any]):
        self.service_type = service_type
        self.config = config
        self.current_capacity = config.get('initial_capacity', 1)
        self.max_capacity = config.get('max_capacity', 10)
        self.min_capacity = config.get('min_capacity', 1)
        self.scaling_threshold = config.get('scaling_threshold', 0.8)
        self.cooldown_period = config.get('cooldown_period', 300)  # 5 Ø¯Ù‚Ø§Ø¦Ù‚
        
        self.metrics = {
            'cpu_usage': 0,
            'memory_usage': 0,
            'request_rate': 0,
            'response_time': 0,
            'error_rate': 0
        }
        
        self.last_scaling_action = 0
        self.scaling_history = []
    
    @abstractmethod
    async def get_current_metrics(self) -> Dict[str, float]:
        """Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""
        pass
    
    @abstractmethod
    async def scale_up(self, additional_instances: int = 1) -> bool:
        """Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø³Ø¹Ø©"""
        pass
    
    @abstractmethod
    async def scale_down(self, instances_to_remove: int = 1) -> bool:
        """ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¹Ø©"""
        pass
    
    def get_scaling_decision(self) -> Dict[str, Any]:
        """Ø§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø± Ø§Ù„ØªÙˆØ³Ø¹Ø©"""
        current_metrics = asyncio.create_task(self.get_current_metrics())
        
        # ØªÙ‚ÙŠÙŠÙ… Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
        should_scale_up = self._should_scale_up(current_metrics)
        should_scale_down = self._should_scale_down(current_metrics)
        
        if should_scale_up and self._can_scale_up():
            return {
                'action': 'scale_up',
                'instances': 1,
                'reason': 'high_load',
                'confidence': self._calculate_confidence(current_metrics, 'up')
            }
        elif should_scale_down and self._can_scale_down():
            return {
                'action': 'scale_down',
                'instances': 1,
                'reason': 'low_load',
                'confidence': self._calculate_confidence(current_metrics, 'down')
            }
        
        return {
            'action': 'maintain',
            'reason': 'normal_load'
        }
    
    def _should_scale_up(self, metrics: Dict[str, float]) -> bool:
        """ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø³Ø¹Ø©"""
        threshold_exceeded = (
            metrics['cpu_usage'] > self.scaling_threshold or
            metrics['memory_usage'] > self.scaling_threshold or
            metrics['request_rate'] > self.config.get('request_threshold', 100) or
            metrics['response_time'] > self.config.get('response_time_threshold', 2.0)
        )
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙØªØ±Ø© Ø§Ù„ØªÙ‡Ø¯Ø¦Ø©
        time_since_last_action = asyncio.get_event_loop().time() - self.last_scaling_action
        
        return threshold_exceeded and time_since_last_action > self.cooldown_period
    
    def _should_scale_down(self, metrics: Dict[str, float]) -> bool:
        """ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¹Ø©"""
        under_utilization = (
            metrics['cpu_usage'] < (self.scaling_threshold * 0.5) and
            metrics['memory_usage'] < (self.scaling_threshold * 0.5) and
            metrics['request_rate'] < (self.config.get('request_threshold', 100) * 0.5)
        )
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø£Ø®Ø·Ø§Ø¡ Ø¹Ø§Ù„ÙŠØ©
        if metrics['error_rate'] > self.config.get('error_threshold', 0.01):
            return False
        
        time_since_last_action = asyncio.get_event_loop().time() - self.last_scaling_action
        
        return under_utilization and time_since_last_action > self.cooldown_period
    
    def _can_scale_up(self) -> bool:
        """ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠÙ…ÙƒÙ† Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø³Ø¹Ø©"""
        return self.current_capacity < self.max_capacity
    
    def _can_scale_down(self) -> bool:
        """ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠÙ…ÙƒÙ† ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¹Ø©"""
        return self.current_capacity > self.min_capacity
    
    def _calculate_confidence(self, metrics: Dict[str, float], direction: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ù‚Ø±Ø§Ø± Ø§Ù„ØªÙˆØ³Ø¹Ø©"""
        if direction == 'up':
            # Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„Ø²ÙŠØ§Ø¯Ø©
            cpu_confidence = min(metrics['cpu_usage'] / self.scaling_threshold, 2.0)
            memory_confidence = min(metrics['memory_usage'] / self.scaling_threshold, 2.0)
            response_confidence = min(metrics['response_time'] / self.config.get('response_time_threshold', 2.0), 2.0)
            
            return (cpu_confidence + memory_confidence + response_confidence) / 3.0
        
        elif direction == 'down':
            # Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ®ÙÙŠØ¶
            cpu_confidence = max(0, (self.scaling_threshold * 0.5 - metrics['cpu_usage']) / (self.scaling_threshold * 0.5))
            memory_confidence = max(0, (self.scaling_threshold * 0.5 - metrics['memory_usage']) / (self.scaling_threshold * 0.5))
            error_confidence = 1.0 - min(metrics['error_rate'] / self.config.get('error_threshold', 0.01), 1.0)
            
            return (cpu_confidence + memory_confidence + error_confidence) / 3.0
        
        return 0.0

class ScalableWebServer(ScalableComponent):
    """Ø®Ø§Ø¯Ù… ÙˆÙŠØ¨ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(ServiceType.WEB_SERVER, config)
        self.servers = []
        self.load_balancer = config.get('load_balancer')
    
    async def get_current_metrics(self) -> Dict[str, float]:
        """Ø¬Ù„Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø®ÙˆØ§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨"""
        if not self.servers:
            return {
                'cpu_usage': 0,
                'memory_usage': 0,
                'request_rate': 0,
                'response_time': 0,
                'error_rate': 0
            }
        
        total_metrics = {
            'cpu_usage': 0,
            'memory_usage': 0,
            'request_rate': 0,
            'response_time': 0,
            'error_rate': 0
        }
        
        # Ø¬Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®ÙˆØ§Ø¯Ù…
        for server in self.servers:
            server_metrics = await self._get_server_metrics(server)
            for key, value in server_metrics.items():
                total_metrics[key] += value
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·
        num_servers = len(self.servers)
        for key in total_metrics:
            total_metrics[key] /= num_servers
        
        return total_metrics
    
    async def _get_server_metrics(self, server: Dict[str, Any]) -> Dict[str, float]:
        """Ø¬Ù„Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø®Ø§Ø¯Ù… ÙˆØ§Ø­Ø¯"""
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        return {
            'cpu_usage': server.get('cpu_usage', 0),
            'memory_usage': server.get('memory_usage', 0),
            'request_rate': server.get('request_rate', 0),
            'response_time': server.get('response_time', 0),
            'error_rate': server.get('error_rate', 0)
        }
    
    async def scale_up(self, additional_instances: int = 1) -> bool:
        """Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø®ÙˆØ§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨"""
        try:
            logger.info(f"Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø®ÙˆØ§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ Ù…Ù† {self.current_capacity} Ø¥Ù„Ù‰ {self.current_capacity + additional_instances}")
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø®ÙˆØ§Ø¯Ù… Ø¬Ø¯ÙŠØ¯Ø©
            for i in range(additional_instances):
                new_server = await self._create_server()
                self.servers.append(new_server)
            
            # ØªØ­Ø¯ÙŠØ« Load Balancer
            if self.load_balancer:
                await self.load_balancer.add_servers([s['id'] for s in self.servers[-additional_instances:]])
            
            self.current_capacity += additional_instances
            self.last_scaling_action = asyncio.get_event_loop().time()
            
            # ØªØ³Ø¬ÙŠÙ„ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®
            self.scaling_history.append({
                'timestamp': self.last_scaling_action,
                'action': 'scale_up',
                'instances': additional_instances,
                'new_capacity': self.current_capacity
            })
            
            return True
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø²ÙŠØ§Ø¯Ø© Ø®ÙˆØ§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨: {e}")
            return False
    
    async def scale_down(self, instances_to_remove: int = 1) -> bool:
        """ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø®ÙˆØ§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨"""
        try:
            if self.current_capacity - instances_to_remove < self.min_capacity:
                logger.warning("Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø¯Ø¯ Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰")
                return False
            
            logger.info(f"ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø®ÙˆØ§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨ Ù…Ù† {self.current_capacity} Ø¥Ù„Ù‰ {self.current_capacity - instances_to_remove}")
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø®ÙˆØ§Ø¯Ù… Ù„Ù„Ø¥Ø²Ø§Ù„Ø© (Ø¢Ø®Ø± Ø®ÙˆØ§Ø¯Ù… ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§)
            servers_to_remove = self.servers[-instances_to_remove:]
            
            # Ø¥Ø²Ø§Ù„Ø© Ù…Ù† Load Balancer Ø£ÙˆÙ„Ø§Ù‹
            if self.load_balancer:
                await self.load_balancer.remove_servers([s['id'] for s in servers_to_remove])
            
            # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø®ÙˆØ§Ø¯Ù…
            for server in servers_to_remove:
                await self._stop_server(server)
            
            # Ø¥Ø²Ø§Ù„Ø© Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©
            self.servers = self.servers[:-instances_to_remove]
            
            self.current_capacity -= instances_to_remove
            self.last_scaling_action = asyncio.get_event_loop().time()
            
            # ØªØ³Ø¬ÙŠÙ„ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®
            self.scaling_history.append({
                'timestamp': self.last_scaling_action,
                'action': 'scale_down',
                'instances': instances_to_remove,
                'new_capacity': self.current_capacity
            })
            
            return True
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ‚Ù„ÙŠÙ„ Ø®ÙˆØ§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨: {e}")
            return False
    
    async def _create_server(self) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø§Ø¯Ù… Ø¬Ø¯ÙŠØ¯"""
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø§Ø¯Ù…
        server_id = f"web_{len(self.servers)}"
        
        server = {
            'id': server_id,
            'status': 'running',
            'cpu_usage': 10,  # Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU Ù…Ù†Ø®ÙØ¶ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
            'memory_usage': 20,  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù†Ø®ÙØ¶
            'request_rate': 0,
            'response_time': 0.1,
            'error_rate': 0,
            'created_at': asyncio.get_event_loop().time()
        }
        
        logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø§Ø¯Ù… ÙˆÙŠØ¨: {server_id}")
        return server
    
    async def _stop_server(self, server: Dict[str, Any]) -> None:
        """Ø¥ÙŠÙ‚Ø§Ù Ø®Ø§Ø¯Ù…"""
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¥ÙŠÙ‚Ø§Ù Ø®Ø§Ø¯Ù…
        logger.info(f"ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø®Ø§Ø¯Ù… ÙˆÙŠØ¨: {server['id']}")

class ScalableDatabase(ScalableComponent):
    """Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙˆØ³Ø¹"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(ServiceType.DATABASE, config)
        self.primary_node = None
        self.replica_nodes = []
        self.connection_pool_size = config.get('connection_pool_size', 10)
    
    async def get_current_metrics(self) -> Dict[str, float]:
        """Ø¬Ù„Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        # Ø¬Ù„Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…Ù† Ø§Ù„Ø¹Ù‚Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        primary_metrics = await self._get_node_metrics(self.primary_node)
        
        # Ø¬Ù„Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…Ù† Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©
        replica_metrics = []
        for replica in self.replica_nodes:
            metrics = await self._get_node_metrics(replica)
            replica_metrics.append(metrics)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª
        total_metrics = primary_metrics.copy()
        
        if replica_metrics:
            for key in total_metrics:
                replica_sum = sum(r[key] for r in replica_metrics)
                total_metrics[key] = (total_metrics[key] + replica_sum) / (len(replica_metrics) + 1)
        
        return total_metrics
    
    async def _get_node_metrics(self, node: Dict[str, Any]) -> Dict[str, float]:
        """Ø¬Ù„Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¹Ù‚Ø¯Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª"""
        return {
            'cpu_usage': node.get('cpu_usage', 0),
            'memory_usage': node.get('memory_usage', 0),
            'disk_usage': node.get('disk_usage', 0),
            'connection_count': node.get('active_connections', 0),
            'query_time': node.get('avg_query_time', 0),
            'cache_hit_ratio': node.get('cache_hit_ratio', 0)
        }
    
    async def scale_up(self, additional_instances: int = 1) -> bool:
        """Ø²ÙŠØ§Ø¯Ø© Ø³Ø¹Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            logger.info(f"Ø¥Ø¶Ø§ÙØ© {additional_instances} Ù†Ø³Ø® Ù…ØªØ·Ø§Ø¨Ù‚Ø© Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            
            # Ø¥Ø¶Ø§ÙØ© Ù†Ø³Ø® Ù…ØªØ·Ø§Ø¨Ù‚Ø© Ø¬Ø¯ÙŠØ¯Ø©
            for i in range(additional_instances):
                new_replica = await self._create_replica_node()
                self.replica_nodes.append(new_replica)
                
                # ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©
                await self._setup_replication(new_replica)
            
            self.current_capacity += additional_instances
            self.last_scaling_action = asyncio.get_event_loop().time()
            
            return True
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ù†Ø³Ø® Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return False
    
    async def scale_down(self, instances_to_remove: int = 1) -> bool:
        """ØªÙ‚Ù„ÙŠÙ„ Ø³Ø¹Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            if len(self.replica_nodes) - instances_to_remove < 0:
                logger.warning("Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø© Ø£ÙƒØ«Ø±")
                return False
            
            logger.info(f"Ø¥Ø²Ø§Ù„Ø© {instances_to_remove} Ù†Ø³Ø® Ù…ØªØ·Ø§Ø¨Ù‚Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ø³Ø® Ù„Ù„Ø¥Ø²Ø§Ù„Ø©
            replicas_to_remove = self.replica_nodes[-instances_to_remove:]
            
            # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù†Ø³Ø®
            for replica in replicas_to_remove:
                await self._stop_replica_node(replica)
            
            # Ø¥Ø²Ø§Ù„Ø© Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©
            self.replica_nodes = self.replica_nodes[:-instances_to_remove]
            
            self.current_capacity -= instances_to_remove
            self.last_scaling_action = asyncio.get_event_loop().time()
            
            return True
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø²Ø§Ù„Ø© Ù†Ø³Ø® Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return False
    
    async def _create_replica_node(self) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù‚Ø¯Ø© Ù†Ø³Ø® Ù…ØªØ·Ø§Ø¨Ù‚Ø© Ø¬Ø¯ÙŠØ¯Ø©"""
        replica_id = f"replica_{len(self.replica_nodes)}"
        
        replica = {
            'id': replica_id,
            'status': 'syncing',
            'cpu_usage': 15,
            'memory_usage': 30,
            'disk_usage': 20,
            'active_connections': 0,
            'avg_query_time': 0.1,
            'cache_hit_ratio': 0.8,
            'lag_behind_primary': 0,
            'created_at': asyncio.get_event_loop().time()
        }
        
        logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù‚Ø¯Ø© Ù†Ø³Ø® Ù…ØªØ·Ø§Ø¨Ù‚Ø©: {replica_id}")
        return replica
    
    async def _stop_replica_node(self, replica: Dict[str, Any]) -> None:
        """Ø¥ÙŠÙ‚Ø§Ù Ø¹Ù‚Ø¯Ø© Ù†Ø³Ø® Ù…ØªØ·Ø§Ø¨Ù‚Ø©"""
        logger.info(f"ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø¹Ù‚Ø¯Ø© Ù†Ø³Ø® Ù…ØªØ·Ø§Ø¨Ù‚Ø©: {replica['id']}")
    
    async def _setup_replication(self, replica: Dict[str, Any]) -> None:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©"""
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ø³Ø®
        logger.info(f"ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©: {replica['id']}")

# Ù…Ø¯ÙŠØ± Ø§Ù„ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø´Ø§Ù…Ù„
class ScalingOrchestrator:
    """Ù…Ù†Ø³Ù‚ Ø§Ù„ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø´Ø§Ù…Ù„"""
    
    def __init__(self, config: Dict[str, Any]):
        self.components = {}
        self.config = config
        self.scaling_policies = self._define_scaling_policies()
        self.monitoring_interval = config.get('monitoring_interval', 60)  # Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ§Ø­Ø¯Ø©
        self.running = False
        
        # Ø±Ø¨Ø· Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        self._initialize_components()
    
    def _define_scaling_policies(self) -> Dict[ServiceType, Dict]:
        """ØªØ¹Ø±ÙŠÙ Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„ØªÙˆØ³Ø¹Ø©"""
        return {
            ServiceType.WEB_SERVER: {
                'strategy': ScalingStrategy.AUTO_SCALING,
                'min_instances': 2,
                'max_instances': 10,
                'scale_up_threshold': 0.7,
                'scale_down_threshold': 0.3,
                'cooldown_period': 300
            },
            ServiceType.API_SERVER: {
                'strategy': ScalingStrategy.AUTO_SCALING,
                'min_instances': 2,
                'max_instances': 8,
                'scale_up_threshold': 0.8,
                'scale_down_threshold': 0.2,
                'cooldown_period': 180
            },
            ServiceType.DATABASE: {
                'strategy': ScalingStrategy.HYBRID,
                'min_replicas': 1,
                'max_replicas': 5,
                'scale_up_threshold': 0.9,
                'scale_down_threshold': 0.4,
                'cooldown_period': 600
            },
            ServiceType.CACHE: {
                'strategy': ScalingStrategy.HORIZONTAL,
                'min_instances': 1,
                'max_instances': 3,
                'scale_up_threshold': 0.85,
                'scale_down_threshold': 0.35,
                'cooldown_period': 300
            }
        }
    
    def _initialize_components(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙˆØ³Ø¹"""
        # Ø®ÙˆØ§Ø¯Ù… Ø§Ù„ÙˆÙŠØ¨
        web_config = {
            'initial_capacity': 2,
            'max_capacity': 10,
            'min_capacity': 2,
            'scaling_threshold': 0.7,
            'cooldown_period': 300
        }
        self.components[ServiceType.WEB_SERVER] = ScalableWebServer(web_config)
        
        # Ø®ÙˆØ§Ø¯Ù… API
        api_config = {
            'initial_capacity': 2,
            'max_capacity': 8,
            'min_capacity': 2,
            'scaling_threshold': 0.8,
            'cooldown_period': 180
        }
        self.components[ServiceType.API_SERVER] = ScalableWebServer(api_config)
        
        # Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        db_config = {
            'initial_capacity': 1,
            'max_capacity': 5,
            'min_capacity': 1,
            'scaling_threshold': 0.9,
            'cooldown_period': 600
        }
        self.components[ServiceType.DATABASE] = ScalableDatabase(db_config)
    
    async def start_monitoring(self):
        """Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙˆØ³Ø¹Ø©"""
        self.running = True
        logger.info("Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙˆØ³Ø¹Ø©")
        
        while self.running:
            try:
                await self._monitor_and_scale()
                await asyncio.sleep(self.monitoring_interval)
            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙˆØ³Ø¹Ø©: {e}")
                await asyncio.sleep(30)
    
    async def _monitor_and_scale(self):
        """Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØªÙˆØ³ÙŠØ¹ Ø§Ù„Ù†Ø¸Ø§Ù…"""
        scaling_decisions = []
        
        # ÙØ­Øµ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        for service_type, component in self.components.items():
            try:
                decision = component.get_scaling_decision()
                if decision['action'] in ['scale_up', 'scale_down']:
                    scaling_decisions.append({
                        'service_type': service_type,
                        'decision': decision
                    })
                    
                    # ØªÙ†ÙÙŠØ° Ù‚Ø±Ø§Ø± Ø§Ù„ØªÙˆØ³Ø¹Ø©
                    await self._execute_scaling_action(service_type, component, decision)
                    
            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ù…ÙƒÙˆÙ† {service_type}: {e}")
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª
        if scaling_decisions:
            logger.info(f"ØªÙ… Ø§ØªØ®Ø§Ø° {len(scaling_decisions)} Ù‚Ø±Ø§Ø±Ø§Øª ØªÙˆØ³Ø¹Ø©")
            for decision in scaling_decisions:
                logger.info(f"Ù‚Ø±Ø§Ø± Ø§Ù„ØªÙˆØ³Ø¹Ø©: {decision['service_type'].value} - {decision['decision']['action']}")
    
    async def _execute_scaling_action(self, service_type: ServiceType, component: ScalableComponent, decision: Dict[str, Any]):
        """ØªÙ†ÙÙŠØ° Ù‚Ø±Ø§Ø± Ø§Ù„ØªÙˆØ³Ø¹Ø©"""
        try:
            if decision['action'] == 'scale_up':
                success = await component.scale_up(decision['instances'])
                if success:
                    logger.info(f"ØªÙ… Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø³Ø¹Ø© Ø¨Ù†Ø¬Ø§Ø­ Ù„Ù€ {service_type.value}")
                else:
                    logger.error(f"ÙØ´Ù„ ÙÙŠ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø³Ø¹Ø© Ù„Ù€ {service_type.value}")
            
            elif decision['action'] == 'scale_down':
                success = await component.scale_down(decision['instances'])
                if success:
                    logger.info(f"ØªÙ… ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¹Ø© Ø¨Ù†Ø¬Ø§Ø­ Ù„Ù€ {service_type.value}")
                else:
                    logger.error(f"ÙØ´Ù„ ÙÙŠ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¹Ø© Ù„Ù€ {service_type.value}")
                    
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†ÙÙŠØ° Ù‚Ø±Ø§Ø± Ø§Ù„ØªÙˆØ³Ø¹Ø© {service_type.value}: {e}")
    
    async def get_system_status(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…"""
        status = {
            'timestamp': asyncio.get_event_loop().time(),
            'components': {},
            'overall_health': 'healthy',
            'scaling_history': []
        }
        
        # Ø¬Ù…Ø¹ Ø­Ø§Ù„Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        for service_type, component in self.components.items():
            try:
                metrics = await component.get_current_metrics()
                
                status['components'][service_type.value] = {
                    'current_capacity': component.current_capacity,
                    'max_capacity': component.max_capacity,
                    'min_capacity': component.min_capacity,
                    'metrics': metrics,
                    'last_scaling_action': component.last_scaling_action,
                    'scaling_history': component.scaling_history[-5:]  # Ø¢Ø®Ø± 5 Ø¹Ù…Ù„ÙŠØ§Øª
                }
                
            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù…Ø¹ Ø­Ø§Ù„Ø© {service_type.value}: {e}")
                status['overall_health'] = 'degraded'
        
        return status
    
    def stop_monitoring(self):
        """Ø¥ÙŠÙ‚Ø§Ù Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙˆØ³Ø¹Ø©"""
        self.running = False
        logger.info("ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙˆØ³Ø¹Ø©")

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¯ÙŠØ± Ø§Ù„ØªÙˆØ³Ø¹Ø©
async def main():
    """ØªØ´ØºÙŠÙ„ Ù…Ø¯ÙŠØ± Ø§Ù„ØªÙˆØ³Ø¹Ø©"""
    config = {
        'monitoring_interval': 30,  # ÙØ­Øµ ÙƒÙ„ 30 Ø«Ø§Ù†ÙŠØ©
        'scaling_policies': {
            'web_server': {
                'min_instances': 2,
                'max_instances': 10,
                'scale_up_threshold': 0.7,
                'scale_down_threshold': 0.3
            }
        }
    }
    
    orchestrator = ScalingOrchestrator(config)
    
    try:
        # Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙˆØ³Ø¹Ø©
        await orchestrator.start_monitoring()
    except KeyboardInterrupt:
        logger.info("ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙˆØ³Ø¹Ø©")
        orchestrator.stop_monitoring()

if __name__ == "__main__":
    asyncio.run(main())
```

## Ø§Ù„ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø£ÙÙ‚ÙŠØ© (Horizontal Scaling)

### Docker Swarm Configuration

```yaml
# docker-stack.yml - ØªÙƒÙˆÙŠÙ† Docker Swarm

version: '3.8'

services:
  app:
    image: saler-app:latest
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/saler
      - REDIS_URL=redis://redis:6379
    volumes:
      - app-logs:/app/logs
      - app-uploads:/app/uploads
    networks:
      - backend
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      placement:
        constraints:
          - node.role == worker
      labels:
        - "com.saler.service=web"

  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=saler
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
    networks:
      - backend
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
      labels:
        - "com.saler.service=database-primary"

  postgres-replica:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=saler
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_replica_data:/var/lib/postgresql/data
    networks:
      - backend
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.role == worker
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
      labels:
        - "com.saler.service=database-replica"

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    networks:
      - backend
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      labels:
        - "com.saler.service=cache"

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/ssl/certs
      - ./uploads:/var/www/uploads
    networks:
      - backend
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.role == worker
      labels:
        - "com.saler.service=load-balancer"

  redis-exporter:
    image: oliver006/redis_exporter:latest
    environment:
      - REDIS_ADDR=redis://redis:6379
    networks:
      - backend
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      labels:
        - "com.saler.service=monitoring"

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    networks:
      - backend
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      labels:
        - "com.saler.service=monitoring"

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    networks:
      - backend
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      labels:
        - "com.saler.service=monitoring"

volumes:
  app-logs:
  app-uploads:
  postgres_data:
  postgres_replica_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  backend:
    driver: overlay
    attachable: true
```

### Kubernetes Deployment

```yaml
# k8s-scaling.yaml - Ù†Ø´Ø± Kubernetes Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹

apiVersion: apps/v1
kind: Deployment
metadata:
  name: saler-app
  namespace: saler
  labels:
    app: saler-app
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: saler-app
  template:
    metadata:
      labels:
        app: saler-app
    spec:
      containers:
      - name: app
        image: saler-app:latest
        ports:
        - containerPort: 3000
        env:
        - name: NODE_ENV
          value: "production"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: database-url
        - name: REDIS_URL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: redis-url
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        volumeMounts:
        - name: app-logs
          mountPath: /app/logs
        - name: app-uploads
          mountPath: /app/uploads
      volumes:
      - name: app-logs
        persistentVolumeClaim:
          claimName: app-logs-pvc
      - name: app-uploads
        persistentVolumeClaim:
          claimName: app-uploads-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: saler-app-service
  namespace: saler
spec:
  selector:
    app: saler-app
  ports:
  - port: 80
    targetPort: 3000
  type: ClusterIP
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: saler-app-hpa
  namespace: saler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: saler-app
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Max
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-primary
  namespace: saler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres-primary
  template:
    metadata:
      labels:
        app: postgres-primary
    spec:
      containers:
      - name: postgres
        image: postgres:15-alpine
        env:
        - name: POSTGRES_DB
          value: saler
        - name: POSTGRES_USER
          value: postgres
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secrets
              key: password
        ports:
        - containerPort: 5432
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        - name: postgres-backups
          mountPath: /backups
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-storage-pvc
      - name: postgres-backups
        persistentVolumeClaim:
          claimName: postgres-backups-pvc
      nodeSelector:
        node-role.kubernetes.io/master: ""
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-replica
  namespace: saler
spec:
  serviceName: postgres-service
  replicas: 2
  selector:
    matchLabels:
      app: postgres-replica
  template:
    metadata:
      labels:
        app: postgres-replica
    spec:
      containers:
      - name: postgres
        image: postgres:15-alpine
        env:
        - name: POSTGRES_DB
          value: saler
        - name: POSTGRES_USER
          value: postgres
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secrets
              key: password
        ports:
        - containerPort: 5432
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1"
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-replica-storage-pvc
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
      storageClassName: ssd
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
  namespace: saler
spec:
  selector:
    app: postgres-primary
  ports:
  - port: 5432
    targetPort: 5432
  clusterIP: None
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: saler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        command: ["redis-server", "--appendonly", "yes"]
        ports:
        - containerPort: 6379
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        volumeMounts:
        - name: redis-storage
          mountPath: /data
      volumes:
      - name: redis-storage
        persistentVolumeClaim:
          claimName: redis-storage-pvc
      nodeSelector:
        node-role.kubernetes.io/master: ""
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
  namespace: saler
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379
  type: ClusterIP
```

### AWS Auto Scaling Configuration

```bash
#!/bin/bash
# aws-autoscaling.sh - ØªÙƒÙˆÙŠÙ† Ø§Ù„ØªÙˆØ³Ø¹Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© ÙÙŠ AWS

echo "ğŸš€ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙˆØ³Ø¹Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© ÙÙŠ AWS"

# Ø¥Ø¹Ø¯Ø§Ø¯ Launch Template
aws ec2 create-launch-template \
    --launch-template-name saler-app-template \
    --launch-template-data '{
        "ImageId": "ami-0abcdef1234567890",
        "InstanceType": "t3.medium",
        "KeyName": "saler-keypair",
        "SecurityGroupIds": ["sg-12345678"],
        "UserData": "'$(base64 -i user-data.sh)'",
        "IamInstanceProfile": {
            "Name": "saler-ec2-role"
        },
        "BlockDeviceMappings": [
            {
                "DeviceName": "/dev/xvda",
                "Ebs": {
                    "VolumeSize": 20,
                    "VolumeType": "gp3",
                    "DeleteOnTermination": true
                }
            }
        ],
        "TagSpecifications": [
            {
                "ResourceType": "instance",
                "Tags": [
                    {
                        "Key": "Name",
                        "Value": "salar-app-instance"
                    },
                    {
                        "Key": "Environment",
                        "Value": "production"
                    }
                ]
            }
        ]
    }'

# Ø¥Ù†Ø´Ø§Ø¡ Auto Scaling Group
aws autoscaling create-auto-scaling-group \
    --auto-scaling-group-name saler-app-asg \
    --launch-template LaunchTemplateName=salar-app-template,Version=1 \
    --min-size 2 \
    --max-size 10 \
    --desired-capacity 3 \
    --vpc-zone-identifier "subnet-12345678,subnet-87654321" \
    --health-check-type EC2 \
    --health-check-grace-period 300 \
    --target-group-arns arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/saler-tg/1234567890abcdef \
    --termination-policies "OldestInstance"

# Ø¥Ø¹Ø¯Ø§Ø¯ Scaling Policies
aws autoscaling put-scaling-policy \
    --policy-name saler-scale-up-policy \
    --auto-scaling-group-name saler-app-asg \
    --policy-type TargetTrackingScaling \
    --target-tracking-configuration '{
        "TargetValue": 70.0,
        "PredefinedMetricSpecification": {
            "PredefinedMetricType": "ASGAverageCPUUtilization"
        },
        "ScaleOutCooldown": 300,
        "ScaleInCooldown": 300
    }'

aws autoscaling put-scaling-policy \
    --policy-name saler-scale-out-policy \
    --auto-scaling-group-name saler-app-asg \
    --policy-type SimpleScaling \
    --adjustment-type ChangeInCapacity \
    --scaling-adjustment 1 \
    --cooldown 300

aws autoscaling put-scaling-policy \
    --policy-name saler-scale-in-policy \
    --auto-scaling-group-name saler-app-asg \
    --policy-type SimpleScaling \
    --adjustment-type ChangeInCapacity \
    --scaling-adjustment -1 \
    --cooldown 600

# Ø¥Ø¹Ø¯Ø§Ø¯ CloudWatch Alerts
aws cloudwatch put-metric-alarm \
    --alarm-name "salar-app-high-cpu" \
    --alarm-description "ØªÙ†Ø¨ÙŠÙ‡ Ø§Ø±ØªÙØ§Ø¹ CPU" \
    --metric-name CPUUtilization \
    --namespace AWS/EC2 \
    --statistic Average \
    --period 300 \
    --threshold 80 \
    --comparison-operator GreaterThanThreshold \
    --evaluation-periods 2 \
    --alarm-actions arn:aws:autoscaling:us-east-1:123456789012:scalingPolicy:abcdef/saler-scale-out-policy

aws cloudwatch put-metric-alarm \
    --alarm-name "salar-app-low-cpu" \
    --alarm-description "ØªÙ†Ø¨ÙŠÙ‡ Ø§Ù†Ø®ÙØ§Ø¶ CPU" \
    --metric-name CPUUtilization \
    --namespace AWS/EC2 \
    --statistic Average \
    --period 600 \
    --threshold 20 \
    --comparison-operator LessThanThreshold \
    --evaluation-periods 3 \
    --alarm-actions arn:aws:autoscaling:us-east-1:123456789012:scalingPolicy:ghijkl/saler-scale-in-policy

echo "âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙˆØ³Ø¹Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!"
```

## ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø­Ù…Ø§Ù„ (Load Balancing)

### Nginx Load Balancer Configuration

```nginx
# nginx-load-balancer.conf
upstream saler_backend {
    # Round Robin (Ø§ÙØªØ±Ø§Ø¶ÙŠ)
    server app1.internal:3000 weight=3 max_fails=3 fail_timeout=30s;
    server app2.internal:3000 weight=2 max_fails=3 fail_timeout=30s;
    server app3.internal:3000 weight=2 max_fails=3 fail_timeout=30s;
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ keepalive
    keepalive 32;
}

upstream saler_api {
    server api1.internal:3001 weight=2 max_fails=3 fail_timeout=30s;
    server api2.internal:3001 weight=2 max_fails=3 fail_timeout=30s;
    server api3.internal:3001 weight=2 max_fails=3 fail_timeout=30s;
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ keepalive
    keepalive 32;
}

upstream saler_websocket {
    server ws1.internal:3002;
    server ws2.internal:3002;
}

# ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø­Ù…Ø§Ù„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Least Connections
upstream saler_least_conn {
    least_conn;
    server app1.internal:3000 max_fails=3 fail_timeout=30s;
    server app2.internal:3000 max_fails=3 fail_timeout=30s;
    server app3.internal:3000 max_fails=3 fail_timeout=30s;
}

# ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø­Ù…Ø§Ù„ Ù…Ø¹ IP Hash
upstream saler_sticky {
    ip_hash;
    server app1.internal:3000;
    server app2.internal:3000;
    server app3.internal:3000;
}

server {
    listen 80;
    server_name saler.app;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name saler.app;

    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª SSL
    ssl_certificate /etc/ssl/certs/saler.crt;
    ssl_certificate_key /etc/ssl/private/saler.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;
    ssl_prefer_server_ciphers off;
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 10m;

    # Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    gzip on;
    gzip_vary on;
    gzip_min_length 10240;
    gzip_proxied expired no-cache no-store private must-revalidate auth;
    gzip_types text/plain text/css text/xml text/javascript application/javascript application/xml+rss application/json;

    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    client_max_body_size 100M;

    # Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=login:10m rate=5r/m;
    limit_req_zone $binary_remote_addr zone=general:10m rate=100r/s;

    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù†
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self'; connect-src 'self' wss: https:;" always;

    # ØªÙˆØ²ÙŠØ¹ Ø£Ø­Ù…Ø§Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    location / {
        limit_req zone=general burst=20 nodelay;
        
        proxy_pass http://saler_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ù‡Ù„Ø©
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø­Ù…Ø§Ù„
        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
        proxy_next_upstream_tries 3;
        proxy_next_upstream_timeout 10s;
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Connection Pooling
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
        proxy_busy_buffers_size 8k;
    }

    # ØªÙˆØ²ÙŠØ¹ Ø£Ø­Ù…Ø§Ù„ API
    location /api/ {
        limit_req zone=api burst=50 nodelay;
        
        proxy_pass http://saler_api;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù€ API
        proxy_connect_timeout 30s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª cache
        proxy_cache api_cache;
        proxy_cache_valid 200 5m;
        proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
        proxy_cache_lock on;
    }

    # Authentication endpoints
    location /api/auth/ {
        limit_req zone=login burst=5 nodelay;
        
        proxy_pass http://saler_api;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # WebSocket connections
    location /socket.io/ {
        proxy_pass http://saler_websocket;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª WebSocket
        proxy_read_timeout 86400;
        proxy_send_timeout 86400;
    }

    # Static files Ù…Ø¹ Cache
    location /static/ {
        alias /var/www/static/;
        expires 1y;
        add_header Cache-Control "public, immutable";
        
        # Ø¶ØºØ· Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ©
        gzip_static on;
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù† Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ©
        location ~* \.(js|css)$ {
            add_header X-Content-Type-Options nosniff;
        }
    }

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª
    location /uploads/ {
        alias /var/www/uploads/;
        expires 1y;
        add_header Cache-Control "public";
        
        # ØªØ­Ø¯ÙŠØ¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø©
        location ~* \.(jpg|jpeg|png|gif|pdf|doc|docx)$ {
            expires 1y;
            add_header Cache-Control "public";
        }
    }

    # Health check endpoint
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
        
        # Ù„Ø§ ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª
        limit_req zone=none;
    }

    # API documentation
    location /docs {
        proxy_pass http://saler_api;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Logging
    access_log /var/log/nginx/access.log combined;
    error_log /var/log/nginx/error.log warn;
}

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù„Ù„Ù€ Blue-Green Deployment
upstream saler_green {
    server green-app1.internal:3000;
    server green-app2.internal:3000;
    server green-app3.internal:3000;
}

upstream saler_blue {
    server blue-app1.internal:3000;
    server blue-app2.internal:3000;
    server blue-app3.internal:3000;
}

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ø´Ø·Ø©
map $cookie_deployment_version $active_upstream {
    default saler_backend;
    "green" saler_green;
    "blue" saler_blue;
}

server {
    listen 443 ssl http2;
    server_name deploy.saler.app;

    location / {
        proxy_pass http://$active_upstream;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

Ù‡Ø°Ø§ Ø¬Ø²Ø¡ Ù…Ù† Ø¯Ù„ÙŠÙ„ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ù†Ø¸Ø§Ù…ØŒ ÙˆÙŠØ´Ù…Ù„:

1. **ØªØµÙ…ÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹**: Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„ØªØµÙ…ÙŠÙ… ÙˆØ§Ù„ØªØ·Ø¨ÙŠÙ‚
2. **Ø§Ù„ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø£ÙÙ‚ÙŠØ©**: Docker Swarm ÙˆKubernetes ÙˆAWS Auto Scaling
3. **ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø­Ù…Ø§Ù„**: Nginx Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„ØªÙˆØ§Ø²Ù†
4. **Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡**: Ù†Ø¸Ø§Ù… Ø´Ø§Ù…Ù„ Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙˆØ³Ø¹Ø©
5. **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠØ©**: Blue-Green ÙˆCanary Deployment

Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:
- ØªØµÙ…ÙŠÙ… Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹ Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
- Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…Ø³ØªÙ…Ø±Ø© Ù„Ù„Ø£Ø¯Ø§Ø¡
- Ø®Ø·Ø· Ø·ÙˆØ§Ø±Ø¦ Ù„Ù„ØªÙˆØ³Ø¹Ø©
- Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ØªØ­Ù…ÙŠÙ„ Ù…Ù†ØªØ¸Ù…Ø©
- ØªØ­Ø³ÙŠÙ† Ù…Ø³ØªÙ…Ø± Ù„Ù„ÙƒÙˆØ¯ ÙˆØ§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©