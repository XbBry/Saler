# Ø¯Ù„ÙŠÙ„ ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø§Ù…Ù„
## Comprehensive Database Performance Optimization Guide

**Ø§Ù„Ø¥ØµØ¯Ø§Ø±:** 2.0.0  
**Ø§Ù„ØªØ§Ø±ÙŠØ®:** 2025-11-04  
**Ø§Ù„Ù†Ø¸Ø§Ù…:** Saler SaaS Platform  

---

## ğŸ“‹ ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª

1. [Ù…Ù‚Ø¯Ù…Ø©](#Ù…Ù‚Ø¯Ù…Ø©)
2. [Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© Ø¹Ù„Ù‰ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ø³ÙŠÙ†](#Ù†Ø¸Ø±Ø©-Ø¹Ø§Ù…Ø©-Ø¹Ù„Ù‰-Ù†Ø¸Ø§Ù…-Ø§Ù„ØªØ­Ø³ÙŠÙ†)
3. [Query Optimization](#query-optimization)
4. [Database Configuration](#database-configuration)
5. [ORM Optimization](#orm-optimization)
6. [Monitoring & Analytics](#monitoring--analytics)
7. [Best Practices](#best-practices)
8. [Ø§Ù„ØµÙŠØ§Ù†Ø© ÙˆØ§Ù„ØªØ´ØºÙŠÙ„](#Ø§Ù„ØµÙŠØ§Ù†Ø©-ÙˆØ§Ù„ØªØ´ØºÙŠÙ„)
9. [Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡](#Ø§Ø³ØªÙƒØ´Ø§Ù-Ø§Ù„Ø£Ø®Ø·Ø§Ø¡)
10. [Ø§Ù„Ù…Ù„Ø§Ø­Ù‚](#Ø§Ù„Ù…Ù„Ø§Ø­Ù‚)

---

## Ù…Ù‚Ø¯Ù…Ø©

### Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„

ÙŠÙ‡Ø¯Ù Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø¯Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ù†ØµØ© Saler SaaSØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰:

- **ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª** - ØªØ­Ù„ÙŠÙ„ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø©
- **Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª** - ØªØ­Ø³ÙŠÙ† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª PostgreSQL Ù„Ù„Ø§ØªØµØ§Ù„ ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡
- **ØªØ­Ø³ÙŠÙ† ORM** - ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª SQLAlchemy ÙˆØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
- **Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„** - Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø´Ø§Ù…Ù„ Ù„Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
- **Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª** - Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ ÙˆØ§Ù„Ø£Ø±Ø´ÙØ©

### Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³Ø¨Ù‚Ø©

- Python 3.9+
- PostgreSQL 13+
- Redis 6+
- SQLAlchemy 1.4+
- FastAPI 0.68+

---

## Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© Ø¹Ù„Ù‰ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ø³ÙŠÙ†

### Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…

ØªÙ… ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ØªØ­Ø³ÙŠÙ† Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª ÙŠØªÙƒÙˆÙ† Ù…Ù†:

```
ğŸ“ app/core/
â”œâ”€â”€ ğŸ—„ï¸ database_optimizer.py          # Ù…Ø­Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
â”œâ”€â”€ ğŸ”§ orm_optimizer.py              # Ù…Ø­Ø³Ù† ORM
â”œâ”€â”€ ğŸ“Š database_monitoring.py        # Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
â”œâ”€â”€ âš™ï¸ database_config.py            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
â”œâ”€â”€ ğŸ¥ advanced_health_check.py      # ÙØ­ÙˆØµØ§Øª Ø§Ù„ØµØ­Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
â”œâ”€â”€ ğŸ—ï¸ database_optimization_migrations.py # migrations Ø§Ù„ØªØ­Ø³ÙŠÙ†
â”œâ”€â”€ ğŸ“ˆ database_performance_dashboard.py # Ù„ÙˆØ­Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
â”œâ”€â”€ ğŸ”„ database_optimization_service.py  # Ø®Ø¯Ù…Ø© Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
â””â”€â”€ ğŸ“‹ database_optimization_guide.md   # Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„
```

### Ø§Ù„Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©

- **ØªØ­Ù„ÙŠÙ„ Ø°ÙƒÙŠ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª** - Ø§ÙƒØªØ´Ø§Ù ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
- **ØªÙˆØµÙŠØ§Øª Ø§Ù„ÙÙ‡Ø§Ø±Ø³** - Ø§Ù‚ØªØ±Ø§Ø­ ÙÙ‡Ø§Ø±Ø³ Ù…Ø®ØµØµØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
- **Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙˆØ±ÙŠØ©** - Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
- **Ù„ÙˆØ­Ø© ØªØ­ÙƒÙ… ØªÙØ§Ø¹Ù„ÙŠØ©** - Ø¹Ø±Ø¶ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª
- **Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ØªÙ„Ù‚Ø§Ø¦ÙŠØ©** - Ù†Ø¸Ø§Ù… Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø°ÙƒÙŠ Ù…Ø¹ Ø§Ù„Ø¶ØºØ· ÙˆØ§Ù„ØªØ´ÙÙŠØ±

---

## Query Optimization

### 1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø©

#### Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„

```python
from app.core.database_optimizer import QueryAnalyzer

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ù„Ù„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
analyzer = QueryAnalyzer()

# ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø·ÙŠØ¡
slow_queries = await analyzer.find_slow_queries(
    threshold_ms=1000,  # Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø£Ø¨Ø·Ø£ Ù…Ù† Ø«Ø§Ù†ÙŠØ© ÙˆØ§Ø­Ø¯Ø©
    limit=50           # Ø£ÙˆÙ„ 50 Ø§Ø³ØªØ¹Ù„Ø§Ù…
)

for query in slow_queries:
    print(f"Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query['query'][:100]}...")
    print(f"ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°: {query['execution_time']:.2f}ms")
    print(f"Ø§Ù„ØªÙƒØ±Ø§Ø±: {query['frequency']} Ù…Ø±Ø©")
    print(f"Ø§Ù„ØªÙˆØµÙŠØ§Øª: {query['recommendations']}")
```

#### ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª

```python
# ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø¹ÙŠÙ†
analysis = await analyzer.analyze_query("""
    SELECT l.*, u.full_name 
    FROM leads l 
    JOIN users u ON l.assigned_to_id = u.id 
    WHERE l.workspace_id = '123' 
    ORDER BY l.created_at DESC
""")

print(f"ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {analysis['estimated_time']:.2f}ms")
print(f"Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {analysis['used_indexes']}")
print(f"Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©: {analysis['suggested_indexes']}")
print(f"Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†: {analysis['optimization_type']}")
```

### 2. ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙÙ‡Ø§Ø±Ø³

#### ÙÙ‡Ø§Ø±Ø³ ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù…Ù‚ØªØ±Ø­Ø©

```python
from app.core.database_optimizer import IndexRecommender

recommender = IndexRecommender()

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ø§Ù„ÙÙ‡Ø§Ø±Ø³
recommendations = await recommender.get_index_recommendations(
    table_name="leads",
    min_usage_frequency=10,
    performance_threshold=1000
)

for rec in recommendations:
    print(f"Ø§Ù„Ø¬Ø¯ÙˆÙ„: {rec['table']}")
    print(f"Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {rec['columns']}")
    print(f"Ù†ÙˆØ¹ Ø§Ù„ÙÙ‡Ø±Ø³: {rec['index_type']}")
    print(f"Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©: {rec['priority']}")
    print(f"Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {rec['estimated_improvement']}%")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³
    await recommender.create_index(
        table=rec['table'],
        columns=rec['columns'],
        index_type=rec['index_type']
    )
```

#### Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø·Ø¨Ù‚Ø© ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…

**Ø¬Ø¯ÙˆÙ„ Users:**
```sql
-- ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
CREATE INDEX ix_users_email ON users(email);
CREATE INDEX ix_users_is_active ON users(is_active);
CREATE INDEX ix_users_role ON users(role);
CREATE INDEX ix_users_last_login_at ON users(last_login_at);
CREATE INDEX ix_users_email_is_active ON users(email, is_active);
CREATE INDEX ix_users_role_last_login ON users(role, last_login_at);
```

**Ø¬Ø¯ÙˆÙ„ Leads:**
```sql
-- ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ù…Ø­ØªÙ…Ù„ÙŠÙ†
CREATE INDEX ix_leads_workspace_status ON leads(workspace_id, status);
CREATE INDEX ix_leads_workspace_temperature ON leads(workspace_id, temperature);
CREATE INDEX ix_leads_workspace_source ON leads(workspace_id, source);
CREATE INDEX ix_leads_assigned_to_created ON leads(assigned_to_id, created_at);
CREATE INDEX ix_leads_workspace_status_created ON leads(workspace_id, status, created_at);
CREATE INDEX ix_leads_source_status ON leads(source, status);
```

**Ø¬Ø¯ÙˆÙ„ Workspaces:**
```sql
-- ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø³Ø§Ø­Ø§Øª Ø§Ù„Ø¹Ù…Ù„
CREATE INDEX ix_workspaces_owner_active ON workspaces(owner_id, plan);
CREATE INDEX ix_workspaces_plan_billing ON workspaces(plan, next_billing_date);
```

### 3. ØªØ­Ø³ÙŠÙ† JOIN Operations

```python
from app.core.database_optimizer import JoinOptimizer

optimizer = JoinOptimizer()

# ØªØ­Ø³ÙŠÙ† JOIN Ù…Ø¹Ù‚Ø¯
optimized_query = await optimizer.optimize_join("""
    SELECT l.*, u.full_name, w.name as workspace_name
    FROM leads l
    LEFT JOIN users u ON l.assigned_to_id = u.id
    LEFT JOIN workspaces w ON l.workspace_id = w.id
    WHERE l.status IN ('NEW', 'CONTACTED')
    ORDER BY l.created_at DESC
""")

print(f"Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†:\n{optimized_query}")

# ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ JOIN
join_analysis = await optimizer.analyze_join_performance(optimized_query)
print(f"Ù†ÙˆØ¹ JOIN Ø§Ù„Ø£Ù…Ø«Ù„: {join_analysis['optimal_join_type']}")
print(f"Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: {join_analysis['required_indexes']}")
```

### 4. ØªØ­Ø³ÙŠÙ† Pagination

```python
from app.core.database_optimizer import PaginationOptimizer

pag_optimizer = PaginationOptimizer()

# ØªØ­Ø³ÙŠÙ† pagination Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
optimized_pagination = await pag_optimizer.optimize_pagination("""
    SELECT l.*, u.full_name 
    FROM leads l 
    JOIN users u ON l.assigned_to_id = u.id 
    WHERE l.workspace_id = ?
    ORDER BY l.created_at DESC
""", page=10, page_size=50)

print(f"Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†:\n{optimized_pagination['query']}")
print(f"Ø§Ù„ØªØ­Ø³ÙŠÙ†: Ø§Ø³ØªØ®Ø¯Ø§Ù… cursor-based pagination Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† OFFSET")
print(f"Ø§Ù„ØªØ­Ø³Ù† Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {optimized_pagination['improvement']}%")
```

---

## Database Configuration

### 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª PostgreSQL

#### Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„

```python
from app.core.database_config import DatabaseConfig

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
config = DatabaseConfig(
    host="localhost",
    port=5432,
    database="saler_db",
    username="saler_user",
    password="secure_password",
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Connection Pool
    pool_size=20,
    max_overflow=30,
    pool_recycle=3600,
    pool_pre_ping=True,
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
    connect_timeout=10,
    command_timeout=30,
    tcp_keepalive=True,
    tcp_keepidle=60,
    tcp_keepcnt=5,
    tcp_keepintvl=30
)

# ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
await config.apply_settings()
```

#### Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©

```python
from app.core.database_config import MemoryOptimizer

memory_optimizer = MemoryOptimizer()

# ØªØ­Ø³ÙŠÙ† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©
memory_settings = {
    "shared_buffers": "256MB",      # 25% Ù…Ù† RAM
    "effective_cache_size": "1GB",  # 75% Ù…Ù† RAM
    "work_mem": "4MB",             # Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ÙØ±Ø¯ÙŠØ©
    "maintenance_work_mem": "64MB", # Ù„Ù„ØµÙŠØ§Ù†Ø© ÙˆØ§Ù„Ù€ VACUUM
    "wal_buffers": "16MB",         # Ù„Ù€ WAL buffering
    "checkpoint_completion_target": 0.9,
    "max_wal_size": "1GB",
    "min_wal_size": "80MB"
}

# ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©
await memory_optimizer.optimize_memory_settings(memory_settings)
```

#### Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Query Planner

```python
# ØªØ­Ø³ÙŠÙ† Ù…Ø®Ø·Ø· Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
query_planner_settings = {
    "random_page_cost": 1.1,      # SSD optimization
    "effective_io_concurrency": 200,  # SSD parallelism
    "cpu_tuple_cost": 0.01,
    "cpu_index_tuple_cost": 0.005,
    "cpu_operator_cost": 0.0025,
    "enable_partitionwise_join": True,
    "enable_partitionwise_aggregate": True,
    "jit": True  # Just-in-time compilation
}

await config.update_query_planner_settings(query_planner_settings)
```

### 2. Connection Pool Management

#### Ù…Ø±Ø§Ù‚Ø¨Ø© Connection Pool

```python
from app.core.database_config import ConnectionPoolManager

pool_manager = ConnectionPoolManager()

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù€ Pool
pool_stats = await pool_manager.get_pool_statistics()

print(f"Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©: {pool_stats['active_connections']}")
print(f"Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ø®Ø§Ù…Ù„Ø©: {pool_stats['idle_connections']}")
print(f"Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ù„Ù‚Ø©: {pool_stats['waiting_connections']}")
print(f"Ù…Ø¹Ø¯Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Pool: {pool_stats['utilization_percent']:.1f}%")

# ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù€ Pool Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if pool_stats['utilization_percent'] > 80:
    await pool_manager.increase_pool_size(additional_connections=10)
    print("ØªÙ… Ø²ÙŠØ§Ø¯Ø© Ø­Ø¬Ù… Connection Pool")
```

#### Ø¥Ø¯Ø§Ø±Ø© Connection Lifecycle

```python
# Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØ¥Ø¯Ø§Ø±Ø© Ø¯ÙˆØ±Ø© Ø­ÙŠØ§Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª
async def monitor_connection_health():
    pool_manager = ConnectionPoolManager()
    
    while True:
        health_status = await pool_manager.check_pool_health()
        
        if health_status['status'] == 'unhealthy':
            # Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„ØªØ§Ù„ÙØ©
            await pool_manager.reset_unhealthy_connections()
        
        if health_status['idle_connections'] > 50:
            # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
            await pool_manager.shrink_pool(target_idle=20)
        
        await asyncio.sleep(60)  # ÙØ­Øµ ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø©

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
asyncio.create_task(monitor_connection_health())
```

---

## ORM Optimization

### 1. ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª SQLAlchemy

#### Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ø­Ø³Ù† ORM

```python
from app.core.orm_optimizer import EagerLoadingOptimizer, QueryOptimizer

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø­Ø³Ù† Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¨ÙƒØ±
eager_optimizer = EagerLoadingOptimizer()

# ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ¹Ù„Ø§Ù… Lead Ù…Ø¹ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
lead_query = session.query(Lead).options(
    selectinload(Lead.assigned_user),
    selectinload(Lead.workspace),
    selectinload(Lead.activities).limit(10),
    joinedload(Lead.messages).limit(5)
).filter(Lead.workspace_id == workspace_id)

# ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
optimized_query = await eager_optimizer.optimize_query(lead_query)

# ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø¹ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
start_time = time.time()
results = optimized_query.all()
execution_time = time.time() - start_time

print(f"ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°: {execution_time:.3f}s")
print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {len(results)}")
```

#### ØªØ­Ø³ÙŠÙ† Relationship Loading

```python
from app.core.orm_optimizer import RelationshipLoader

rel_loader = RelationshipLoader()

# Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ØªØ­Ù…ÙŠÙ„ Ù…ØªØ¯Ø±Ø¬Ø© Ù„Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
loading_strategies = {
    'Lead': {
        'assigned_user': 'selectin',    # ØªØ­Ù…ÙŠÙ„ Ù…Ø¨ÙƒØ± Ù„Ù„Ø¹Ù„Ø§Ù‚Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        'workspace': 'joined',          # JOIN Ù„Ù„Ø¹Ù„Ø§Ù‚Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
        'activities': 'subquery',       # ØªØ­Ù…ÙŠÙ„ ÙØ±Ø¹ÙŠ Ù„Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©
        'messages': 'selectin_limit',   # ØªØ­Ù…ÙŠÙ„ Ù…Ø¨ÙƒØ± Ù…Ø¹ Ø­Ø¯
    },
    'User': {
        'workspaces': 'selectin',       # ØªØ­Ù…ÙŠÙ„ Ù…Ø¨ÙƒØ± Ù„Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        'assigned_leads': 'lazy',       # ØªØ­Ù…ÙŠÙ„ ÙƒØ³ÙˆÙ„ Ù„Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
    }
}

# ØªØ·Ø¨ÙŠÙ‚ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„
optimized_queries = await rel_loader.apply_loading_strategies(
    base_queries=base_queries,
    strategies=loading_strategies
)
```

#### ØªØ­Ø³ÙŠÙ† Query Results

```python
from app.core.orm_optimizer import QueryResultOptimizer

result_optimizer = QueryResultOptimizer()

# ØªØ­Ø³ÙŠÙ† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
async def get_optimized_leads(workspace_id: str, limit: int = 50):
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
    query = session.query(Lead).filter(
        Lead.workspace_id == workspace_id
    ).order_by(Lead.created_at.desc())
    
    # ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    optimized_result = await result_optimizer.optimize_result_set(
        query=query,
        limit=limit,
        include_count=True,
        cache_key=f"leads_{workspace_id}_{limit}",
        cache_ttl=300  # 5 Ø¯Ù‚Ø§Ø¦Ù‚
    )
    
    return optimized_result

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
leads_data = await get_optimized_leads("workspace_123", limit=100)
print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ù…Ø­ØªÙ…Ù„ÙŠÙ†: {leads_data['total_count']}")
print(f"Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {len(leads_data['items'])}")
```

### 2. Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø®ØµØµØ©

#### Batch Loading Ù„Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©

```python
async def load_user_leads_batch(user_ids: List[str]):
    """ØªØ­Ù…ÙŠÙ„ Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù€ IDs Ø¥Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
    batch_size = 50
    batches = [user_ids[i:i+batch_size] for i in range(0, len(user_ids), batch_size)]
    
    all_leads = []
    
    for batch in batches:
        # ØªØ­Ù…ÙŠÙ„ Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©
        batch_leads = session.query(Lead).filter(
            Lead.assigned_to_id.in_(batch),
            Lead.workspace_id == current_workspace_id
        ).options(
            selectinload(Lead.assigned_user),
            selectinload(Lead.workspace)
        ).all()
        
        all_leads.extend(batch_leads)
    
    return all_leads

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
user_ids = ["user1", "user2", "user3"]
leads = await load_user_leads_batch(user_ids)
```

#### Dynamic Eager Loading

```python
async def get_leads_with_flexible_loading(
    lead_ids: List[str],
    include_relations: List[str] = None
):
    """ØªØ­Ù…ÙŠÙ„ Ù…Ø±Ù† Ù„Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©"""
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    if include_relations is None:
        include_relations = ['assigned_user', 'workspace']
    
    # Ø¨Ù†Ø§Ø¡ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¨ÙƒØ±
    eager_options = []
    
    if 'assigned_user' in include_relations:
        eager_options.append(selectinload(Lead.assigned_user))
    
    if 'workspace' in include_relations:
        eager_options.append(joinedload(Lead.workspace))
    
    if 'activities' in include_relations:
        eager_options.append(
            selectinload(Lead.activities).limit(10)
        )
    
    # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
    query = session.query(Lead).filter(
        Lead.id.in_(lead_ids)
    ).options(*eager_options)
    
    return query.all()
```

---

## Monitoring & Analytics

### 1. Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡

#### Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©

```python
from app.core.database_monitoring import QueryMonitor, PerformanceCollector

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø±Ø§Ù‚Ø¨ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
monitor = QueryMonitor()

# Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
await monitor.start_monitoring()

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø­Ø§Ù„ÙŠØ©
current_metrics = await monitor.get_current_metrics()

print(f"Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {current_metrics['avg_query_time']:.2f}ms")
print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª/Ø«Ø§Ù†ÙŠØ©: {current_metrics['queries_per_second']:.2f}")
print(f"Ø§ØªØµØ§Ù„Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©: {current_metrics['active_connections']}")
print(f"Ù…Ø¹Ø¯Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {current_metrics['memory_usage_percent']:.1f}%")
```

#### Ø¬Ù…Ø¹ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

```python
from app.core.database_monitoring import MetricsAggregator

aggregator = MetricsAggregator()

# Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙØªØ±Ø© Ø²Ù…Ù†ÙŠØ©
time_range = {
    "start": datetime.now() - timedelta(hours=24),
    "end": datetime.now()
}

# ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙŠÙˆÙ…ÙŠ
daily_analysis = await aggregator.analyze_performance_metrics(time_range)

print(f"Ø£Ø¯Ø§Ø¡ Ø¢Ø®Ø± 24 Ø³Ø§Ø¹Ø©:")
print(f"  - Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {daily_analysis['avg_query_time']:.2f}ms")
print(f"  - Ø£Ø¨Ø·Ø£ Ø§Ø³ØªØ¹Ù„Ø§Ù…: {daily_analysis['slowest_query']['query'][:100]}...")
print(f"  - ÙˆÙ‚Øª Ø£Ø¨Ø·Ø£ Ø§Ø³ØªØ¹Ù„Ø§Ù…: {daily_analysis['slowest_query']['time']:.2f}ms")
print(f"  - Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹: {daily_analysis['most_frequent']['query'][:100]}...")
print(f"  - Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {daily_analysis['most_frequent']['count']}")

# ØªØ­Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø©
slow_query_analysis = await aggregator.analyze_slow_queries(
    threshold_ms=1000,
    time_range=time_range
)

for query in slow_query_analysis:
    print(f"\nØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…:")
    print(f"  - Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query['query'][:150]}...")
    print(f"  - ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù…ØªÙˆØ³Ø·: {query['avg_time']:.2f}ms")
    print(f"  - Ø¹Ø¯Ø¯ Ø§Ù„ØªÙ†ÙÙŠØ°Ø§Øª: {query['execution_count']}")
    print(f"  - Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙˆÙ‚Øª: {query['total_time']:.2f}ms")
    print(f"  - Ø§Ù„ØªÙˆØµÙŠØ§Øª: {query['recommendations']}")
```

### 2. Ù„ÙˆØ­Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©

#### Ø¹Ø±Ø¶ Ù„ÙˆØ­Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡

```python
from app.core.database_performance_dashboard import PerformanceDashboard

dashboard = PerformanceDashboard()

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙˆØ­Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
dashboard_data = await dashboard.get_dashboard_data()

print("=== Ù„ÙˆØ­Ø© Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===")
print(f"Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø©: {dashboard_data['overall_status']}")
print(f"ÙˆÙ‚Øª Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: {dashboard_data['last_updated']}")

# Ù…Ø¤Ø´Ø±Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©
print(f"\nğŸ“Š Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:")
for metric in dashboard_data['key_metrics']:
    print(f"  - {metric['name']}: {metric['value']} {metric['unit']}")
    print(f"    Ø§Ù„ØªØºÙŠÙŠØ±: {metric['change']} ({metric['trend']})")

# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø©
print(f"\nğŸŒ Ø£Ø¨Ø·Ø£ 5 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:")
for query in dashboard_data['slow_queries']:
    print(f"  - {query['query'][:100]}... ({query['avg_time']:.2f}ms)")

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙÙ‡Ø§Ø±Ø³
print(f"\nğŸ” Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙÙ‡Ø§Ø±Ø³:")
for index in dashboard_data['index_usage']:
    print(f"  - {index['name']}: {index['hits']} Ø§Ø³ØªØ®Ø¯Ø§Ù…ØŒ {index['efficiency']:.1f}% ÙƒÙØ§Ø¡Ø©")
```

#### Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø§Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ù…Ø®ØµØµØ©

```python
from app.core.database_performance_dashboard import PerformanceReporter

reporter = PerformanceReporter()

# Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ø´Ø§Ù…Ù„
report = await reporter.generate_performance_report(
    report_type="weekly",
    include_recommendations=True,
    format="html"
)

# Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
report_file = f"performance_report_{datetime.now().strftime('%Y%m%d')}.html"
with open(report_file, 'w', encoding='utf-8') as f:
    f.write(report)

print(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡: {report_file}")

# Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± CSV Ù„Ù„ØªØ­Ù„ÙŠÙ„
csv_report = await reporter.generate_csv_report(
    metrics=["query_time", "connection_count", "cache_hit_ratio"],
    time_range="last_7_days"
)

csv_file = f"metrics_{datetime.now().strftime('%Y%m%d')}.csv"
with open(csv_file, 'w', newline='', encoding='utf-8') as f:
    f.write(csv_report)

print(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± CSV: {csv_file}")
```

### 3. Health Checks Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©

#### ÙØ­Øµ ØµØ­Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

```python
from app.core.advanced_health_check import AdvancedHealthChecker

health_checker = AdvancedHealthChecker()

# ÙØ­Øµ Ø´Ø§Ù…Ù„ Ù„ØµØ­Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
health_status = await health_checker.comprehensive_health_check()

print(f"Ø­Ø§Ù„Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {health_status['overall_status']}")
print(f"ÙˆÙ‚Øª Ø§Ù„ÙØ­Øµ: {health_status['timestamp']}")

# ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙØ­ÙˆØµØ§Øª
print(f"\nØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙØ­ÙˆØµØ§Øª:")
for check_name, check_result in health_status['checks'].items():
    status_icon = "âœ…" if check_result['status'] == 'healthy' else "âŒ"
    print(f"  {status_icon} {check_name}: {check_result['status']}")
    if check_result['details']:
        print(f"    Ø§Ù„ØªÙØ§ØµÙŠÙ„: {check_result['details']}")
    if check_result['recommendations']:
        print(f"    Ø§Ù„ØªÙˆØµÙŠØ§Øª: {check_result['recommendations']}")

# Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù€ Connection Pool
pool_health = await health_checker.check_connection_pool_health()
print(f"\nğŸ”— Connection Pool:")
print(f"  - Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©: {pool_health['active']}/{pool_health['total']}")
print(f"  - Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: {pool_health['utilization_percent']:.1f}%")
print(f"  - Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ù„Ù‚Ø©: {pool_health['waiting']}")

# ØªØ­Ù„ÙŠÙ„ ØµØ­Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
table_health = await health_checker.analyze_table_health()
print(f"\nğŸ“‹ ØµØ­Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„:")
for table, stats in table_health.items():
    print(f"  - {table}:")
    print(f"    Ø§Ù„Ø³Ø¬Ù„Ø§Øª: {stats['row_count']:,}")
    print(f"    Ø§Ù„Ø­Ø¬Ù…: {stats['size_mb']:.2f} MB")
    print(f"    Ø§Ù„ÙÙ‡Ø§Ø±Ø³: {stats['index_count']}")
    print(f"    Ø¢Ø®Ø± ØªØ­Ù„ÙŠÙ„: {stats['last_analyzed']}")
```

---

## Best Practices

### 1. Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ

#### Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ

```python
import asyncio
from datetime import datetime, timedelta

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
backup_dir = Path("/var/backups/saler")
backup_dir.mkdir(parents=True, exist_ok=True)

async def automated_backup():
    """Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
    
    # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
    from scripts.automated_database_backup import DatabaseBackupManager, BackupConfig
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
    config = BackupConfig(
        database_url=os.getenv("DATABASE_URL"),
        backup_directory=str(backup_dir),
        retention_days=30,           # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ 30 ÙŠÙˆÙ…
        compress=True,               # Ø¶ØºØ· Ø§Ù„Ù…Ù„ÙØ§Øª
        encrypt=True,                # ØªØ´ÙÙŠØ± Ø§Ù„Ù…Ù„ÙØ§Øª
        s3_backup=True,             # Ø±ÙØ¹ Ø¥Ù„Ù‰ S3
        s3_bucket=os.getenv("S3_BUCKET_NAME"),
        s3_prefix="database-backups/",
        notification_webhook=os.getenv("SLACK_WEBHOOK_URL")
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
    backup_manager = DatabaseBackupManager(config)
    
    try:
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        backup_info = await backup_manager.create_backup()
        
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­")
        print(f"Ù…Ø¹Ø±Ù Ø§Ù„Ù†Ø³Ø®Ø©: {backup_info['backup_id']}")
        print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª: {len(backup_info['files'])}")
        print(f"Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {backup_info['total_size_mb']} MB")
        
        return backup_info
        
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {e}")
        raise

# Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
async def schedule_backups():
    """Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©"""
    
    while True:
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙƒÙ„ 6 Ø³Ø§Ø¹Ø§Øª
            await automated_backup()
            
            # Ø§Ù†ØªØ¸Ø§Ø± 6 Ø³Ø§Ø¹Ø§Øª (21600 Ø«Ø§Ù†ÙŠØ©)
            await asyncio.sleep(21600)
            
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ: {e}")
            # Ø§Ù†ØªØ¸Ø§Ø± Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø© Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰
            await asyncio.sleep(3600)

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
asyncio.create_task(schedule_backups())
```

#### Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø§Ù„ÙŠØ¯ÙˆÙŠ

```bash
# Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙÙˆØ±ÙŠØ©
cd /workspace/saler/backend
python scripts/automated_database_backup.py backup

# Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©
python scripts/automated_database_backup.py list

# Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
python scripts/automated_database_backup.py restore --backup-id 20251104_120000

# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
python scripts/automated_database_backup.py cleanup
```

### 2. Ø£Ø±Ø´ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©

#### Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø£Ø±Ø´ÙØ©

```python
from datetime import datetime, timedelta

async def archive_old_data():
    """Ø£Ø±Ø´ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    
    from app.core.database import get_db_session
    
    async with get_db_session() as session:
        
        # ØªØ­Ø¯ÙŠØ¯ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø£Ø±Ø´ÙØ© (Ø£ÙƒØ«Ø± Ù…Ù† Ø³Ù†Ø©)
        archive_date = datetime.now() - timedelta(days=365)
        
        # Ø£Ø±Ø´ÙØ© Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        archived_activities = await session.execute(
            text("""
                WITH archived_activities AS (
                    DELETE FROM activities 
                    WHERE created_at < :archive_date
                    RETURNING *
                )
                INSERT INTO activities_archive 
                SELECT * FROM archived_activities
            """),
            {"archive_date": archive_date}
        )
        
        # Ø£Ø±Ø´ÙØ© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        archived_messages = await session.execute(
            text("""
                WITH archived_messages AS (
                    DELETE FROM messages 
                    WHERE created_at < :archive_date 
                    AND status = 'READ'
                    RETURNING *
                )
                INSERT INTO messages_archive 
                SELECT * FROM archived_messages
            """),
            {"archive_date": archive_date}
        )
        
        # Ø£Ø±Ø´ÙØ© Ø£Ø­Ø¯Ø§Ø« Ø§Ù„ÙˆÙŠØ¨ Ù‡ÙˆÙƒ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        archived_webhooks = await session.execute(
            text("""
                WITH archived_webhooks AS (
                    DELETE FROM webhook_events 
                    WHERE created_at < :archive_date 
                    AND status = 'COMPLETED'
                    RETURNING *
                )
                INSERT INTO webhook_events_archive 
                SELECT * FROM archived_webhooks
            """),
            {"archive_date": archive_date}
        )
        
        print(f"ØªÙ… Ø£Ø±Ø´ÙØ©:")
        print(f"  - {archived_activities.rowcount} Ù†Ø´Ø§Ø· Ù‚Ø¯ÙŠÙ…")
        print(f"  - {archived_messages.rowcount} Ø±Ø³Ø§Ù„Ø© Ù‚Ø¯ÙŠÙ…Ø©")
        print(f"  - {archived_webhooks.rowcount} Ø­Ø¯Ø« ÙˆÙŠØ¨ Ù‡ÙˆÙƒ Ù‚Ø¯ÙŠÙ…")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø¹Ø¯ Ø§Ù„Ø£Ø±Ø´ÙØ©
        await analyze_table_sizes()
        
        await session.commit()

async def analyze_table_sizes():
    """ØªØ­Ù„ÙŠÙ„ Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø¹Ø¯ Ø§Ù„Ø£Ø±Ø´ÙØ©"""
    
    async with get_db_session() as session:
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
        result = await session.execute(text("""
            SELECT 
                schemaname,
                tablename,
                n_tup_ins as inserts,
                n_tup_upd as updates,
                n_tup_del as deletes,
                n_live_tup as live_tuples,
                n_dead_tup as dead_tuples
            FROM pg_stat_user_tables 
            ORDER BY n_live_tup DESC
        """))
        
        tables_stats = result.fetchall()
        
        print("\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø¹Ø¯ Ø§Ù„Ø£Ø±Ø´ÙØ©:")
        for stat in tables_stats:
            dead_tuples_percent = (stat.dead_tuples / max(stat.live_tuples, 1)) * 100
            print(f"  - {stat.tablename}:")
            print(f"    Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø­ÙŠØ©: {stat.live_tuples:,}")
            print(f"    Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…ÙŠØªØ©: {stat.dead_tuples:,} ({dead_tuples_percent:.1f}%)")
            
            if dead_tuples_percent > 20:
                print(f"    âš ï¸ ÙŠØ­ØªØ§Ø¬ VACUUM - Ù†Ø³Ø¨Ø© Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…ÙŠØªØ© Ø¹Ø§Ù„ÙŠØ©")
```

### 3. ØµÙŠØ§Ù†Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

#### Ø¬Ø¯ÙˆÙ„Ø© Ù…Ù‡Ø§Ù… Ø§Ù„ØµÙŠØ§Ù†Ø©

```python
async def database_maintenance():
    """Ù…Ù‡Ø§Ù… ØµÙŠØ§Ù†Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¯ÙˆØ±ÙŠØ©"""
    
    from app.core.database import get_db_session
    
    async with get_db_session() as session:
        
        print("ğŸ”§ Ø¨Ø¯Ø¡ ØµÙŠØ§Ù†Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        
        # 1. VACUUM Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø³Ø§Ø­Ø©
        print("Ø¬Ø§Ø±ÙŠ VACUUM...")
        await session.execute(text("VACUUM ANALYZE"))
        
        # 2. Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø¹Ø·Ù„Ø©
        print("Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø§Ø±Ø³...")
        await session.execute(text("""
            REINDEX DATABASE saler_db CONCURRENTLY
        """))
        
        # 3. ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø®Ø·Ø·
        print("Ø¬Ø§Ø±ÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª...")
        await session.execute(text("""
            ANALYZE
        """))
        
        # 4. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        print("Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        integrity_check = await session.execute(text("""
            SELECT 
                schemaname,
                tablename,
                hasindexes,
                hasrules,
                hastriggers
            FROM pg_tables 
            WHERE schemaname = 'public'
        """))
        
        for table in integrity_check:
            print(f"  âœ… {table.tablename}: ÙÙ‡Ø§Ø±Ø³={table.hasindexes}, Ù‚ÙˆØ§Ø¹Ø¯={table.hasrules}, Ù…Ø­ÙØ²Ø§Øª={table.hastriggers}")
        
        await session.commit()
        print("âœ… ØªÙ…Øª ØµÙŠØ§Ù†Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")

async def schedule_maintenance():
    """Ø¬Ø¯ÙˆÙ„Ø© Ù…Ù‡Ø§Ù… Ø§Ù„ØµÙŠØ§Ù†Ø©"""
    
    while True:
        try:
            # ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙŠØ§Ù†Ø© ÙƒÙ„ ÙŠÙˆÙ… Ø£Ø­Ø¯ ÙÙŠ Ø§Ù„Ø³Ø§Ø¹Ø© 2:00 ØµØ¨Ø§Ø­Ø§Ù‹
            now = datetime.now()
            next_sunday = now + timedelta(days=(6 - now.weekday()) % 7)
            next_maintenance = next_sunday.replace(hour=2, minute=0, second=0, microsecond=0)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ
            wait_seconds = (next_maintenance - now).total_seconds()
            
            print(f"â° Ø³ÙŠØªÙ… ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ: {next_maintenance}")
            print(f"â³ Ø§Ù†ØªØ¸Ø§Ø± {wait_seconds/3600:.1f} Ø³Ø§Ø¹Ø©...")
            
            await asyncio.sleep(wait_seconds)
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙŠØ§Ù†Ø©
            await database_maintenance()
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„ØµÙŠØ§Ù†Ø©: {e}")
            await asyncio.sleep(3600)  # Ø§Ù†ØªØ¸Ø§Ø± Ø³Ø§Ø¹Ø© Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰

# ØªØ´ØºÙŠÙ„ Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„ØµÙŠØ§Ù†Ø©
asyncio.create_task(schedule_maintenance())
```

---

## Ø§Ù„ØµÙŠØ§Ù†Ø© ÙˆØ§Ù„ØªØ´ØºÙŠÙ„

### 1. Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©

#### Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª

```python
from app.core.database_monitoring import AlertManager

alert_manager = AlertManager()

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø­Ø¯ÙˆØ¯ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡
alert_thresholds = {
    "query_time": {
        "warning": 1000,    # ØªØ­Ø°ÙŠØ± Ø¹Ù†Ø¯ 1 Ø«Ø§Ù†ÙŠØ©
        "critical": 5000,   # Ø®Ø·ÙŠØ± Ø¹Ù†Ø¯ 5 Ø«ÙˆØ§Ù†
    },
    "connection_usage": {
        "warning": 80,      # ØªØ­Ø°ÙŠØ± Ø¹Ù†Ø¯ 80%
        "critical": 95,     # Ø®Ø·ÙŠØ± Ø¹Ù†Ø¯ 95%
    },
    "memory_usage": {
        "warning": 85,      # ØªØ­Ø°ÙŠØ± Ø¹Ù†Ø¯ 85%
        "critical": 95,     # Ø®Ø·ÙŠØ± Ø¹Ù†Ø¯ 95%
    },
    "disk_usage": {
        "warning": 80,      # ØªØ­Ø°ÙŠØ± Ø¹Ù†Ø¯ 80%
        "critical": 90,     # Ø®Ø·ÙŠØ± Ø¹Ù†Ø¯ 90%
    }
}

# ØªØ³Ø¬ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡
alert_rules = [
    {
        "name": "Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø·ÙŠØ¦Ø©",
        "condition": "avg_query_time > 2000",
        "duration": "5m",
        "severity": "critical",
        "actions": ["email", "slack"]
    },
    {
        "name": "Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¹Ø§Ù„ÙŠ",
        "condition": "connection_usage > 90",
        "duration": "2m",
        "severity": "warning",
        "actions": ["slack"]
    },
    {
        "name": "Ø°Ø§ÙƒØ±Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù…ØªÙ„Ø¦Ø©",
        "condition": "database_memory > 95",
        "duration": "1m",
        "severity": "critical",
        "actions": ["email", "slack", "pagerduty"]
    }
]

await alert_manager.configure_alerts(alert_thresholds, alert_rules)
```

#### Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©

```python
async def performance_monitoring_loop():
    """Ø­Ù„Ù‚Ø© Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©"""
    
    while True:
        try:
            # Ø¬Ù…Ø¹ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            current_metrics = await get_current_performance_metrics()
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø­Ø±Ø¬Ø©
            critical_issues = []
            
            if current_metrics['avg_query_time'] > 5000:
                critical_issues.append(f"Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø·ÙŠØ¦Ø©: {current_metrics['avg_query_time']:.2f}ms")
            
            if current_metrics['connection_usage'] > 95:
                critical_issues.append(f"Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø¹Ø§Ù„ÙŠ: {current_metrics['connection_usage']:.1f}%")
            
            if current_metrics['active_connections'] > current_metrics['max_connections'] * 0.9:
                critical_issues.append(f"Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª ØªÙ‚ØªØ±Ø¨ Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰")
            
            # Ø¥Ø±Ø³Ø§Ù„ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„Ø­Ø±Ø¬Ø©
            if critical_issues:
                await alert_manager.send_critical_alerts(
                    title="Ù…Ø´Ø§ÙƒÙ„ Ø­Ø±Ø¬Ø© ÙÙŠ Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
                    details=critical_issues,
                    metrics=current_metrics
                )
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª
            trend_analysis = await analyze_performance_trends(time_range="1h")
            
            if trend_analysis['deteriorating']:
                await alert_manager.send_warning_alert(
                    title="ØªØ¯Ù‡ÙˆØ± ÙÙŠ Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
                    details=f"Ø§ØªØ¬Ø§Ù‡ ØªØ¯Ù‡ÙˆØ±: {trend_analysis['trend_description']}",
                    recommendations=trend_analysis['recommendations']
                )
            
            await asyncio.sleep(60)  # ÙØ­Øµ ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø©
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
            await asyncio.sleep(60)

# ØªØ´ØºÙŠÙ„ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
asyncio.create_task(performance_monitoring_loop())
```

### 2. ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ

#### Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ

```python
from app.core.database_optimization_service import OptimizationService

async def automatic_optimization():
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
    
    optimization_service = OptimizationService()
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙƒÙ„ Ø³Ø§Ø¹Ø©
    while True:
        try:
            print("ğŸ”„ Ø¨Ø¯Ø¡ Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ...")
            
            # 1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            new_slow_queries = await optimization_service.analyze_new_slow_queries()
            
            # 2. Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø§Ø±Ø³ Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
            new_indexes = await optimization_service.create_suggested_indexes()
            
            # 3. ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            await optimization_service.refresh_query_statistics()
            
            # 4. ØªØ­Ø³ÙŠÙ† Connection Pool
            pool_optimization = await optimization_service.optimize_connection_pool()
            
            # 5. ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            cache_cleanup = await optimization_service.cleanup_old_cache_entries()
            
            print(f"âœ… Ø§Ù†ØªÙ‡Øª Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ†:")
            print(f"  - Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø·ÙŠØ¦Ø© Ø¬Ø¯ÙŠØ¯Ø©: {len(new_slow_queries)}")
            print(f"  - ÙÙ‡Ø§Ø±Ø³ Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù†Ø´Ø£Ø©: {len(new_indexes)}")
            print(f"  - ØªØ­Ø³ÙŠÙ† Pool: {pool_optimization}")
            print(f"  - Ø¹Ù†Ø§ØµØ± Cache Ù…Ø­Ø°ÙˆÙØ©: {cache_cleanup}")
            
            await asyncio.sleep(3600)  # ÙƒÙ„ Ø³Ø§Ø¹Ø©
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ: {e}")
            await asyncio.sleep(300)  # Ø§Ù†ØªØ¸Ø§Ø± 5 Ø¯Ù‚Ø§Ø¦Ù‚ Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰

# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
asyncio.create_task(automatic_optimization())
```

---

## Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡

### 1. ØªØ´Ø®ÙŠØµ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡

#### ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø©

```python
async def diagnose_slow_queries():
    """ØªØ´Ø®ÙŠØµ Ø£Ø³Ø¨Ø§Ø¨ Ø¨Ø·Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª"""
    
    from app.core.database_monitoring import QueryAnalyzer
    
    analyzer = QueryAnalyzer()
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¨Ø·Ø£ 10 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
    slow_queries = await analyzer.get_slowest_queries(limit=10)
    
    for i, query in enumerate(slow_queries, 1):
        print(f"\nğŸ” Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… #{i}:")
        print(f"Ø§Ù„ÙˆÙ‚Øª: {query['execution_time']:.2f}ms")
        print(f"Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query['query'][:200]}...")
        print(f"Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„: {query['tables']}")
        print(f"Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {query['indexes_used']}")
        
        # ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ø¨Ø·Ø¡
        diagnosis = await analyzer.diagnose_slow_query(query['query'])
        
        print(f"Ø§Ù„ØªØ´Ø®ÙŠØµ:")
        print(f"  - Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: {diagnosis['issue_type']}")
        print(f"  - Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ù…Ø­ØªÙ…Ù„: {diagnosis['root_cause']}")
        print(f"  - Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©:")
        for solution in diagnosis['solutions']:
            print(f"    â€¢ {solution}")
        
        # ØªÙˆØµÙŠØ§Øª Ø§Ù„ÙÙ‡Ø§Ø±Ø³
        index_recommendations = diagnosis.get('index_recommendations', [])
        if index_recommendations:
            print(f"ÙÙ‡Ø§Ø±Ø³ Ù…Ù‚ØªØ±Ø­Ø©:")
            for idx in index_recommendations:
                print(f"  - {idx}")
```

#### ÙØ­Øµ ØµØ­Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

```python
async def comprehensive_health_diagnostic():
    """ØªØ´Ø®ÙŠØµ Ø´Ø§Ù…Ù„ Ù„ØµØ­Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    
    from app.core.advanced_health_check import AdvancedHealthChecker
    
    checker = AdvancedHealthChecker()
    
    # ÙØ­Øµ Ø´Ø§Ù…Ù„
    health_report = await checker.generate_health_report()
    
    print("ğŸ¥ ØªÙ‚Ø±ÙŠØ± ØµØ­Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø§Ù…Ù„:")
    print(f"Ø§Ù„ØªÙˆÙ‚ÙŠØª: {health_report['timestamp']}")
    print(f"Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø©: {health_report['overall_status']}")
    
    # ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„
    connection_check = health_report['checks']['connection']
    print(f"\nğŸ”— Ø§Ù„Ø§ØªØµØ§Ù„:")
    print(f"  - Ø§Ù„Ø­Ø§Ù„Ø©: {connection_check['status']}")
    print(f"  - Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {connection_check['response_time']:.2f}ms")
    print(f"  - Ø­Ø¬Ù… Ø§Ù„Ø§ØªØµØ§Ù„: {connection_check['connection_pool_size']}")
    
    # ÙØ­Øµ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    memory_check = health_report['checks']['memory']
    print(f"\nğŸ’¾ Ø§Ù„Ø°Ø§ÙƒØ±Ø©:")
    print(f"  - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {memory_check['usage_percent']:.1f}%")
    print(f"  - Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {memory_check['used_mb']:.2f} MB")
    print(f"  - Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…ØªØ§Ø­Ø©: {memory_check['available_mb']:.2f} MB")
    
    # ÙØ­Øµ Ø§Ù„ÙÙ‡Ø§Ø±Ø³
    index_check = health_report['checks']['indexes']
    print(f"\nğŸ” Ø§Ù„ÙÙ‡Ø§Ø±Ø³:")
    print(f"  - Ø¹Ø¯Ø¯ Ø§Ù„ÙÙ‡Ø§Ø±Ø³: {index_check['total_indexes']}")
    print(f"  - ÙÙ‡Ø§Ø±Ø³ ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©: {index_check['unused_indexes']}")
    print(f"  - ÙÙ‡Ø§Ø±Ø³ Ù…Ø¹Ø·Ù„Ø©: {index_check['corrupted_indexes']}")
    
    if index_check['unused_indexes'] > 0:
        print(f"  âš ï¸ ÙÙ‡Ø§Ø±Ø³ ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø© (ÙŠÙ…ÙƒÙ† Ø­Ø°ÙÙ‡Ø§ Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©):")
        for idx in index_check['unused_details']:
            print(f"    - {idx}")
    
    # ÙØ­Øµ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
    table_check = health_report['checks']['tables']
    print(f"\nğŸ“‹ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„:")
    print(f"  - Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„: {table_check['total_tables']}")
    print(f"  - Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø­Ø§Ø¬Ø© ØªØ­Ù„ÙŠÙ„: {table_check['tables_needing_analyze']}")
    
    if table_check['tables_needing_analyze']:
        print(f"  âš ï¸ Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø­Ø§Ø¬Ø© ANALYZE:")
        for table in table_check['tables_list']:
            print(f"    - {table}")
    
    # Ø§Ù„ØªÙˆØµÙŠØ§Øª
    recommendations = health_report['recommendations']
    if recommendations:
        print(f"\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
        for rec in recommendations:
            print(f"  - {rec}")
    
    return health_report
```

### 2. Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©

#### Ø¥ØµÙ„Ø§Ø­ Connection Pool Issues

```python
async def fix_connection_pool_issues():
    """Ø¥ØµÙ„Ø§Ø­ Ù…Ø´Ø§ÙƒÙ„ Connection Pool"""
    
    from app.core.database_config import ConnectionPoolManager
    
    pool_manager = ConnectionPoolManager()
    
    # ÙØ­Øµ Ø­Ø§Ù„Ø© Pool
    pool_status = await pool_manager.get_pool_status()
    
    if pool_status['utilization_percent'] > 90:
        print("âš ï¸ Connection Pool Ù…ÙƒØªØ¸ØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥ØµÙ„Ø§Ø­...")
        
        # Ø²ÙŠØ§Ø¯Ø© Ø­Ø¬Ù… Pool
        await pool_manager.increase_pool_size(
            additional_connections=10
        )
        print("âœ… ØªÙ… Ø²ÙŠØ§Ø¯Ø© Ø­Ø¬Ù… Connection Pool")
    
    if pool_status['waiting_connections'] > 5:
        print("âš ï¸ Ø§ØªØµØ§Ù„Ø§Øª Ù…Ø¹Ù„Ù‚Ø©ØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥ØµÙ„Ø§Ø­...")
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ù„Ù‚Ø©
        await pool_manager.reset_waiting_connections()
        print("âœ… ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ù„Ù‚Ø©")
    
    if pool_status['idle_connections'] > 50:
        print("âš ï¸ Ø§ØªØµØ§Ù„Ø§Øª Ø®Ø§Ù…Ù„Ø© ÙƒØ«ÙŠØ±Ø©ØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙ†Ø¸ÙŠÙ...")
        
        # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        await pool_manager.shrink_pool(target_idle=20)
        print("âœ… ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©")
    
    # ÙØ­Øµ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø·Ù„Ø©
    dead_connections = await pool_manager.find_dead_connections()
    if dead_connections:
        print(f"âš ï¸ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(dead_connections)} Ø§ØªØµØ§Ù„ Ù…Ø¹Ø·Ù„")
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø·Ù„Ø©
        await pool_manager.cleanup_dead_connections()
        print("âœ… ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø·Ù„Ø©")

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¥ØµÙ„Ø§Ø­
await fix_connection_pool_issues()
```

#### ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø¹Ø·Ù„Ø©

```python
async def repair_corrupted_indexes():
    """Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø¹Ø·Ù„Ø©"""
    
    from app.core.database_optimizer import IndexRecommender
    
    recommender = IndexRecommender()
    
    # ÙØ­Øµ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø¹Ø·Ù„Ø©
    corrupted_indexes = await recommender.find_corrupted_indexes()
    
    if not corrupted_indexes:
        print("âœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø¨Ø­Ø§Ù„Ø© Ø¬ÙŠØ¯Ø©")
        return
    
    print(f"âš ï¸ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(corrupted_indexes)} ÙÙ‡Ø±Ø³ Ù…Ø¹Ø·Ù„")
    
    # Ø¥ØµÙ„Ø§Ø­ ÙƒÙ„ ÙÙ‡Ø±Ø³ Ù…Ø¹Ø·Ù„
    for index_info in corrupted_indexes:
        table_name = index_info['table']
        index_name = index_info['index_name']
        
        print(f"ğŸ”§ Ø¥ØµÙ„Ø§Ø­ ÙÙ‡Ø±Ø³: {index_name} Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø¯ÙˆÙ„: {table_name}")
        
        try:
            # Ø­Ø°Ù Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹Ø·Ù„
            await recommender.drop_index(index_name, table_name)
            
            # Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³
            await recommender.recreate_index(
                table=table_name,
                columns=index_info['columns'],
                index_type=index_info['index_type'],
                index_name=index_name
            )
            
            print(f"âœ… ØªÙ… Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙÙ‡Ø±Ø³: {index_name}")
            
        except Exception as e:
            print(f"âŒ ÙØ´Ù„ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙÙ‡Ø±Ø³ {index_name}: {e}")
    
    # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
    await recommender.analyze_all_tables()
    print("âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„")

# ØªØ´ØºÙŠÙ„ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙÙ‡Ø§Ø±Ø³
await repair_corrupted_indexes()
```

---

## Ø§Ù„Ù…Ù„Ø§Ø­Ù‚

### 1. Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

```bash
# Ù…ØªØºÙŠØ±Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
DATABASE_URL=postgresql://username:password@localhost:5432/saler_db

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
BACKUP_DIRECTORY=/var/backups/saler
BACKUP_RETENTION_DAYS=30
BACKUP_COMPRESS=true
BACKUP_ENCRYPT=true
S3_BACKUP_ENABLED=true
S3_BUCKET_NAME=saler-database-backups
S3_BACKUP_PREFIX=database-backups/

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
EMAIL_ALERT_RECIPIENTS=admin@saler.com,tech@saler.com

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
MONITORING_ENABLED=true
PERFORMANCE_THRESHOLD_MS=1000
ALERT_CHECK_INTERVAL_SECONDS=60
```

### 2. Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ÙÙŠØ¯Ø©

```bash
# Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©
psql -d saler_db -c "
SELECT count(*) as active_connections, state 
FROM pg_stat_activity 
GROUP BY state;
"

# Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø£Ø¨Ø·Ø£
psql -d saler_db -c "
SELECT query, mean_exec_time, calls 
FROM pg_stat_statements 
ORDER BY mean_exec_time DESC 
LIMIT 10;
"

# ÙØ­Øµ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙÙ‡Ø§Ø±Ø³
psql -d saler_db -c "
SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;
"

# ÙØ­Øµ Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
psql -d saler_db -c "
SELECT schemaname, tablename, pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
"

# ÙØ­Øµ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
psql -d saler_db -c "
SELECT query, calls, total_exec_time, mean_exec_time, stddev_exec_time
FROM pg_stat_statements
ORDER BY total_exec_time DESC
LIMIT 5;
"
```

### 3. Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©

#### Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ…Ø±

```bash
#!/bin/bash
# monitor_database_performance.sh

DATABASE_URL="postgresql://user:pass@localhost:5432/saler_db"

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
mkdir -p /var/log/saler/database
LOG_FILE="/var/log/saler/database/performance_$(date +%Y%m%d).log"

while true; do
    TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S')
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©
    ACTIVE_CONN=$(psql "$DATABASE_URL" -t -c "SELECT count(*) FROM pg_stat_activity WHERE state = 'active';" | xargs)
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
    AVG_QUERY_TIME=$(psql "$DATABASE_URL" -t -c "SELECT mean_exec_time FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 1;" | xargs)
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠØ©
    QPS=$(psql "$DATABASE_URL" -t -c "SELECT sum(calls) FROM pg_stat_statements;" | xargs)
    
    # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø³Ø¬Ù„
    echo "$TIMESTAMP - Active Connections: $ACTIVE_CONN, Avg Query Time: ${AVG_QUERY_TIME}ms, QPS: $QPS" >> "$LOG_FILE"
    
    # ÙØ­Øµ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø­Ø±Ø¬Ø©
    if (( $(echo "$AVG_QUERY_TIME > 2000" | bc -l) )); then
        echo "ALERT: Slow queries detected - Avg time: ${AVG_QUERY_TIME}ms" >> "$LOG_FILE"
    fi
    
    if (( ACTIVE_CONN > 80 )); then
        echo "ALERT: High connection usage - Active: $ACTIVE_CONN" >> "$LOG_FILE"
    fi
    
    sleep 60  # ÙØ­Øµ ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø©
done
```

#### ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©

```bash
#!/bin/bash
# cleanup_old_logs.sh

# Ø­Ø°Ù Ø³Ø¬Ù„Ø§Øª Ø£Ù‚Ø¯Ù… Ù…Ù† 30 ÙŠÙˆÙ…
find /var/log/saler/database -name "*.log" -mtime +30 -delete

# Ø¶ØºØ· Ø³Ø¬Ù„Ø§Øª Ø£Ù‚Ø¯Ù… Ù…Ù† 7 Ø£ÙŠØ§Ù…
find /var/log/saler/database -name "*.log" -mtime +7 ! -name "*.gz" -exec gzip {} \;

echo "ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙÙŠ $(date)"
```

### 4. Docker Compose Ù„Ù„ØªØ­Ø³ÙŠÙ†

```yaml
# docker-compose.optimization.yml

version: '3.8'

services:
  saler-postgres-optimized:
    image: postgres:14
    environment:
      POSTGRES_DB: saler_db
      POSTGRES_USER: saler_user
      POSTGRES_PASSWORD: secure_password
    command: >
      postgres
        -c shared_buffers=256MB
        -c effective_cache_size=1GB
        -c work_mem=4MB
        -c maintenance_work_mem=64MB
        -c wal_buffers=16MB
        -c checkpoint_completion_target=0.9
        -c max_wal_size=1GB
        -c min_wal_size=80MB
        -c random_page_cost=1.1
        -c effective_io_concurrency=200
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/var/backups/saler
    ports:
      - "5432:5432"
    restart: unless-stopped

  saler-monitoring:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    restart: unless-stopped

  saler-grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    restart: unless-stopped

volumes:
  postgres_data:
  prometheus_data:
  grafana_data:
```

### 5. Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ ÙˆØ§Ù„Ø¯Ø¹Ù…

- **Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ:** tech@saler.com
- **Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚:** https://docs.saler.com/performance
- **Ø§Ù„Ù€ GitHub:** https://github.com/saler/database-optimization
- **Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ:** https://support.saler.com

---

## Ø§Ù„Ø®Ù„Ø§ØµØ©

ØªÙ… ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ù†ØµØ© Saler SaaS Ø¨Ù†Ø¬Ø§Ø­. Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØºØ·ÙŠ Ø¬Ù…ÙŠØ¹ Ø¬ÙˆØ§Ù†Ø¨ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙŠØ§Ù†Ø© ÙˆØ§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ…Ø±.

### Ø§Ù„Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ù…Ø­Ù‚Ù‚Ø©:

âœ… **ØªØ­Ù„ÙŠÙ„ Ø°ÙƒÙŠ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª** - Ø§ÙƒØªØ´Ø§Ù ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹  
âœ… **ÙÙ‡Ø§Ø±Ø³ Ù…Ø­Ø³Ù†Ø©** - 36 ÙÙ‡Ø±Ø³ Ø¬Ø¯ÙŠØ¯ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡  
âœ… **Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙˆØ±ÙŠØ©** - Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø´Ø§Ù…Ù„ Ù„Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª  
âœ… **Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ØªÙ„Ù‚Ø§Ø¦ÙŠØ©** - Ù†Ø¸Ø§Ù… Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø°ÙƒÙŠ Ù…Ø¹ Ø§Ù„ØªØ´ÙÙŠØ± ÙˆØ§Ù„Ø¶ØºØ·  
âœ… **Ù…Ø±ÙˆÙ†Ø© ÙˆÙ‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹** - ØªØµÙ…ÙŠÙ… Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹ Ù…Ø¹ Ù†Ù…Ùˆ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª  

### Ø§Ù„ØªÙˆØµÙŠØ§Øª Ù„Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©:

1. **ØªØ·Ø¨ÙŠÙ‚ Migration Ø§Ù„Ø¬Ø¯ÙŠØ¯** - ØªØ´ØºÙŠÙ„ Ø§Ù„Ù€ migration Ù„Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙÙ‡Ø§Ø±Ø³
2. **ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©** - ØªØ´ØºÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØ§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©
3. **Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©** - ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©
4. **ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ±ÙŠÙ‚** - ØªØ¯Ø±ÙŠØ¨ ÙØ±ÙŠÙ‚ Ø§Ù„ØªØ·ÙˆÙŠØ± Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù…
5. **Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¯ÙˆØ±ÙŠØ©** - Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø£Ø³Ø¨ÙˆØ¹ÙŠØ§Ù‹ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ø¸Ø§Ù…

---

**ğŸ“ Ù„Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª:** tech@saler.com  
**ğŸ“š Ù„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª:** https://docs.saler.com/database-optimization