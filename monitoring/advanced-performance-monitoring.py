#!/usr/bin/env python3
"""
Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Advanced Performance Monitoring System
Ù†Ø¸Ø§Ù… Ø´Ø§Ù…Ù„ Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ ÙˆØ§Ù„ØªÙ†Ø¨Ø¤Ø§Øª ÙˆØ§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©
Advanced and comprehensive system for monitoring application performance with intelligent analytics, predictions, and auto-optimization
"""

import asyncio
import aiohttp
import psutil
import time
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
from enum import Enum
import numpy as np
from scipy import stats
import threading
import queue
import sqlite3
import redis
import prometheus_client
from prometheus_client import Gauge, Counter, Histogram, Summary
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class PerformanceMetric(Enum):
    """Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
    RESPONSE_TIME = "response_time"
    THROUGHPUT = "throughput"
    ERROR_RATE = "error_rate"
    CPU_USAGE = "cpu_usage"
    MEMORY_USAGE = "memory_usage"
    DISK_IO = "disk_io"
    NETWORK_IO = "network_io"
    DATABASE_CONNECTIONS = "db_connections"
    CACHE_HIT_RATE = "cache_hit_rate"
    PAGE_LOAD_TIME = "page_load_time"
    TIME_TO_FIRST_BYTE = "ttfb"
    DOM_READY_TIME = "dom_ready_time"

class AlertThreshold(Enum):
    """Ø­Ø¯ÙˆØ¯ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

@dataclass
class PerformanceDataPoint:
    """Ù†Ù‚Ø·Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    timestamp: datetime
    metric: PerformanceMetric
    value: float
    unit: str
    labels: Dict[str, str]
    metadata: Dict[str, Any] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

@dataclass
class PerformanceAlert:
    """ØªÙ†Ø¨ÙŠÙ‡ Ø£Ø¯Ø§Ø¡"""
    id: str
    metric: PerformanceMetric
    severity: AlertThreshold
    message: str
    current_value: float
    threshold: float
    timestamp: datetime
    trend: str  # "increasing", "decreasing", "stable"
    recommendations: List[str]

@dataclass
class OptimizationRecommendation:
    """ØªÙˆØµÙŠØ© ØªØ­Ø³ÙŠÙ†"""
    id: str
    category: str
    title: str
    description: str
    impact: str  # "high", "medium", "low"
    effort: str  # "high", "medium", "low"
    expected_improvement: float
    implementation_steps: List[str]
    automation_possible: bool

class PerformanceAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø°ÙƒÙŠ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.data_buffer = defaultdict(lambda: deque(maxlen=1000))
        self.metrics = self.setup_metrics()
        self.alert_thresholds = self.get_default_thresholds()
        self.ml_models = self.setup_ml_models()
        self.logger = self.setup_logging()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯ÙˆØ±ÙŠ
        self.analysis_interval = config.get('analysis_interval', 60)  # Ø«Ø§Ù†ÙŠØ©
        self.prediction_window = config.get('prediction_window', 3600)  # Ø«Ø§Ù†ÙŠØ© (1 Ø³Ø§Ø¹Ø©)
        
    def setup_logging(self) -> logging.Logger:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ø³Ø¬Ù„Ø§Øª"""
        logger = logging.getLogger('PerformanceAnalyzer')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def setup_metrics(self) -> Dict[str, Any]:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§ÙŠÙŠØ³ Prometheus"""
        metrics = {
            'response_time': Histogram(
                'app_response_time_seconds',
                'Application response time',
                ['endpoint', 'method'],
                buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
            ),
            'throughput': Gauge(
                'app_throughput_requests_per_second',
                'Application throughput (requests per second)',
                ['endpoint']
            ),
            'error_rate': Gauge(
                'app_error_rate_percent',
                'Application error rate (%)',
                ['endpoint', 'status_code']
            ),
            'cpu_usage': Gauge(
                'system_cpu_usage_percent',
                'System CPU usage (%)'
            ),
            'memory_usage': Gauge(
                'system_memory_usage_percent',
                'System memory usage (%)'
            ),
            'disk_io': Gauge(
                'system_disk_io_bytes_per_second',
                'System disk I/O (bytes/sec)'
            ),
            'network_io': Gauge(
                'system_network_io_bytes_per_second',
                'System network I/O (bytes/sec)'
            ),
            'db_connections': Gauge(
                'database_connections_active',
                'Active database connections',
                ['database']
            ),
            'cache_hit_rate': Gauge(
                'cache_hit_rate_percent',
                'Cache hit rate (%)',
                ['cache_type']
            )
        }
        
        return metrics
    
    def get_default_thresholds(self) -> Dict[PerformanceMetric, Dict[str, float]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª"""
        return {
            PerformanceMetric.RESPONSE_TIME: {
                'low': 1.0,      # 1 Ø«Ø§Ù†ÙŠØ©
                'medium': 2.0,   # 2 Ø«Ø§Ù†ÙŠØ©
                'high': 5.0,     # 5 Ø«ÙˆØ§Ù†
                'critical': 10.0  # 10 Ø«ÙˆØ§Ù†
            },
            PerformanceMetric.CPU_USAGE: {
                'low': 50.0,     # 50%
                'medium': 70.0,  # 70%
                'high': 85.0,    # 85%
                'critical': 95.0  # 95%
            },
            PerformanceMetric.MEMORY_USAGE: {
                'low': 60.0,     # 60%
                'medium': 75.0,  # 75%
                'high': 85.0,    # 85%
                'critical': 95.0  # 95%
            },
            PerformanceMetric.ERROR_RATE: {
                'low': 1.0,      # 1%
                'medium': 3.0,   # 3%
                'high': 5.0,     # 5%
                'critical': 10.0  # 10%
            },
            PerformanceMetric.DISK_IO: {
                'low': 1000000,      # 1 MB/s
                'medium': 10000000,  # 10 MB/s
                'high': 50000000,    # 50 MB/s
                'critical': 100000000  # 100 MB/s
            },
            PerformanceMetric.NETWORK_IO: {
                'low': 1000000,      # 1 MB/s
                'medium': 10000000,  # 10 MB/s
                'high': 50000000,    # 50 MB/s
                'critical': 100000000  # 100 MB/s
            }
        }
    
    def setup_ml_models(self) -> Dict[str, Any]:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ"""
        models = {
            'anomaly_detector': IsolationForest(
                contamination=0.1,
                random_state=42
            ),
            'trend_predictor': StandardScaler(),
            'threshold_optimizer': None  # Ø³ÙŠØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹
        }
        
        return models
    
    def collect_performance_data(self) -> List[PerformanceDataPoint]:
        """Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        data_points = []
        timestamp = datetime.now()
        
        # Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        try:
            # CPU
            cpu_percent = psutil.cpu_percent(interval=1)
            data_points.append(PerformanceDataPoint(
                timestamp=timestamp,
                metric=PerformanceMetric.CPU_USAGE,
                value=cpu_percent,
                unit="percent",
                labels={}
            ))
            
            # Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            memory = psutil.virtual_memory()
            data_points.append(PerformanceDataPoint(
                timestamp=timestamp,
                metric=PerformanceMetric.MEMORY_USAGE,
                value=memory.percent,
                unit="percent",
                labels={}
            ))
            
            # Ø§Ù„Ù‚Ø±Øµ
            disk_io = psutil.disk_io_counters()
            if disk_io:
                data_points.append(PerformanceDataPoint(
                    timestamp=timestamp,
                    metric=PerformanceMetric.DISK_IO,
                    value=disk_io.read_bytes + disk_io.write_bytes,
                    unit="bytes",
                    labels={}
                ))
            
            # Ø§Ù„Ø´Ø¨ÙƒØ©
            network_io = psutil.net_io_counters()
            if network_io:
                data_points.append(PerformanceDataPoint(
                    timestamp=timestamp,
                    metric=PerformanceMetric.NETWORK_IO,
                    value=network_io.bytes_sent + network_io.bytes_recv,
                    unit="bytes",
                    labels={}
                ))
        
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…: {e}")
        
        # Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ (Ù…Ù† Redis/Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)
        try:
            app_metrics = self.collect_application_metrics()
            data_points.extend(app_metrics)
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚: {e}")
        
        return data_points
    
    def collect_application_metrics(self) -> List[PerformanceDataPoint]:
        """Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚"""
        data_points = []
        timestamp = datetime.now()
        
        try:
            # Ù…Ù† Redis
            if hasattr(self, 'redis_client') and self.redis_client:
                redis_info = self.redis_client.info()
                
                # Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©
                data_points.append(PerformanceDataPoint(
                    timestamp=timestamp,
                    metric=PerformanceMetric.DATABASE_CONNECTIONS,
                    value=redis_info.get('connected_clients', 0),
                    unit="count",
                    labels={'database': 'redis'}
                ))
                
                # Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¶Ø±Ø¨ (Hit Rate)
                hits = redis_info.get('keyspace_hits', 0)
                misses = redis_info.get('keyspace_misses', 0)
                total_requests = hits + misses
                hit_rate = (hits / total_requests * 100) if total_requests > 0 else 0
                
                data_points.append(PerformanceDataPoint(
                    timestamp=timestamp,
                    metric=PerformanceMetric.CACHE_HIT_RATE,
                    value=hit_rate,
                    unit="percent",
                    labels={'cache_type': 'redis'}
                ))
        
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚: {e}")
        
        return data_points
    
    def add_data_point(self, data_point: PerformanceDataPoint):
        """Ø¥Ø¶Ø§ÙØ© Ù†Ù‚Ø·Ø© Ø¨ÙŠØ§Ù†Ø§Øª"""
        # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        self.data_buffer[data_point.metric].append(data_point)
        
        # ØªØ­Ø¯ÙŠØ« Ù…Ù‚Ø§ÙŠÙŠØ³ Prometheus
        if data_point.metric in self.metrics:
            if hasattr(self.metrics[data_point.metric], 'set'):
                self.metrics[data_point.metric].set(data_point.value)
            elif hasattr(self.metrics[data_point.metric], 'observe'):
                # Ù„Ù„Ù€ Histogram
                self.metrics[data_point.metric].observe(data_point.value)
        
        # ØªØ­Ù„ÙŠÙ„ ÙÙˆØ±ÙŠ Ù„Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª
        self.check_immediate_alerts(data_point)
    
    def check_immediate_alerts(self, data_point: PerformanceDataPoint):
        """ÙØ­Øµ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„ÙÙˆØ±ÙŠØ©"""
        if data_point.metric not in self.alert_thresholds:
            return
        
        thresholds = self.alert_thresholds[data_point.metric]
        current_value = data_point.value
        
        severity = None
        if current_value >= thresholds['critical']:
            severity = AlertThreshold.CRITICAL
        elif current_value >= thresholds['high']:
            severity = AlertThreshold.HIGH
        elif current_value >= thresholds['medium']:
            severity = AlertThreshold.MEDIUM
        elif current_value >= thresholds['low']:
            severity = AlertThreshold.LOW
        
        if severity:
            alert = PerformanceAlert(
                id=f"alert_{int(time.time())}_{data_point.metric.value}",
                metric=data_point.metric,
                severity=severity,
                message=f"{data_point.metric.value} Ù‚ÙŠÙ…Ø© {current_value} ØªØ¬Ø§ÙˆØ²Øª Ø§Ù„Ø­Ø¯ {severity.value}",
                current_value=current_value,
                threshold=thresholds[severity.value],
                timestamp=data_point.timestamp,
                trend=self.analyze_trend(data_point.metric),
                recommendations=self.generate_recommendations(data_point.metric, current_value)
            )
            
            self.handle_alert(alert)
    
    def analyze_trend(self, metric: PerformanceMetric) -> str:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡"""
        if len(self.data_buffer[metric]) < 10:
            return "unknown"
        
        # Ø£Ø®Ø° Ø¢Ø®Ø± 10 Ù†Ù‚Ø§Ø·
        recent_data = [dp.value for dp in list(self.data_buffer[metric])[-10:]]
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ
        x = list(range(len(recent_data)))
        slope, _, r_value, p_value, _ = stats.linregress(x, recent_data)
        
        if abs(slope) < 0.1:  # ØªØºÙŠÙŠØ± Ø·ÙÙŠÙ
            return "stable"
        elif slope > 0:
            return "increasing"
        else:
            return "decreasing"
    
    def generate_recommendations(self, metric: PerformanceMetric, current_value: float) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†"""
        recommendations = []
        
        if metric == PerformanceMetric.CPU_USAGE:
            if current_value > 80:
                recommendations.extend([
                    "ØªØ­Ø³ÙŠÙ† Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ CPU",
                    "Ø¥Ø¶Ø§ÙØ© horizontal scaling",
                    "Ø§Ø³ØªØ®Ø¯Ø§Ù… caching Ù„Ù„ØªÙ‚Ù„ÙŠÙ„ Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©"
                ])
        
        elif metric == PerformanceMetric.MEMORY_USAGE:
            if current_value > 80:
                recommendations.extend([
                    "ØªØ­Ø³ÙŠÙ† Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚",
                    "Ø¥Ø¶Ø§ÙØ© garbage collection tuning",
                    "ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­ØªÙØ¸ Ø¨Ù‡Ø§ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"
                ])
        
        elif metric == PerformanceMetric.RESPONSE_TIME:
            if current_value > 5:
                recommendations.extend([
                    "ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
                    "Ø¥Ø¶Ø§ÙØ© database indexing",
                    "Ø§Ø³ØªØ®Ø¯Ø§Ù… CDN Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ©",
                    "ØªØ­Ø³ÙŠÙ† caching strategy"
                ])
        
        elif metric == PerformanceMetric.ERROR_RATE:
            if current_value > 5:
                recommendations.extend([
                    "ÙØ­Øµ logs Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡",
                    "ØªØ­Ø³ÙŠÙ† error handling",
                    "Ø¥Ø¶Ø§ÙØ© retries Ù„Ù„Ù€ API calls",
                    "ÙØ­Øµ dependencies ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª"
                ])
        
        return recommendations
    
    def handle_alert(self, alert: PerformanceAlert):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡"""
        self.logger.warning(f"ØªÙ†Ø¨ÙŠÙ‡ Ø£Ø¯Ø§Ø¡: {alert.message}")
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡
        self.save_alert(alert)
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø¹Ø§Ø± (ÙŠÙ…ÙƒÙ† Ø±Ø¨Ø·Ù‡ Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª)
        self.send_alert_notification(alert)
    
    def save_alert(self, alert: PerformanceAlert):
        """Ø­ÙØ¸ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡"""
        # ÙŠÙ…ÙƒÙ† Ø­ÙØ¸Ù‡ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ Redis
        alert_data = asdict(alert)
        # Ù…Ø«Ø§Ù„: redis_client.set(f"alert:{alert.id}", json.dumps(alert_data))
    
    def send_alert_notification(self, alert: PerformanceAlert):
        """Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø¹Ø§Ø± Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡"""
        # ÙŠÙ…ÙƒÙ† Ø±Ø¨Ø·Ù‡ Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
        # Ù…Ø«Ù„ Ø¥Ø±Ø³Ø§Ù„ emailØŒ SlackØŒ DiscordØŒ etc.
        pass
    
    def detect_anomalies(self) -> List[PerformanceAlert]:
        """ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ"""
        anomalies = []
        
        for metric, data_points in self.data_buffer.items():
            if len(data_points) < 50:  # Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„
                continue
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            values = [dp.value for dp in data_points[-100:]]  # Ø¢Ø®Ø± 100 Ù†Ù‚Ø·Ø©
            
            try:
                # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
                anomalies_detected = self.ml_models['anomaly_detector'].fit_predict(
                    np.array(values).reshape(-1, 1)
                )
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                for i, is_anomaly in enumerate(anomalies_detected):
                    if is_anomaly == -1:  # Ø´Ø°ÙˆØ°
                        recent_value = values[i]
                        trend = self.analyze_trend(metric)
                        
                        alert = PerformanceAlert(
                            id=f"anomaly_{int(time.time())}_{metric.value}_{i}",
                            metric=metric,
                            severity=AlertThreshold.HIGH,
                            message=f"ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø´Ø°ÙˆØ° ÙÙŠ {metric.value}: {recent_value}",
                            current_value=recent_value,
                            threshold=0,  # ØºÙŠØ± Ù…Ø­Ø¯Ø¯ Ù„Ù„Ø´Ø°ÙˆØ°
                            timestamp=data_points[-100 + i].timestamp,
                            trend=trend,
                            recommendations=self.generate_anomaly_recommendations(metric, recent_value)
                        )
                        
                        anomalies.append(alert)
            
            except Exception as e:
                self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ù„Ù€ {metric}: {e}")
        
        return anomalies
    
    def generate_anomaly_recommendations(self, metric: PerformanceMetric, value: float) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù„Ù„Ø´Ø°ÙˆØ°"""
        recommendations = [
            "ØªØ­Ù‚Ù‚ Ù…Ù† logs Ø§Ù„Ù†Ø¸Ø§Ù… Ù„Ù„ØªÙØ§ØµÙŠÙ„",
            "ÙØ­Øµ usage patterns ÙÙŠ Ø¢Ø®Ø± 24 Ø³Ø§Ø¹Ø©",
            "Ø±Ø§Ø¬Ø¹ recent deployments Ø£Ùˆ changes",
            "ÙØ­Øµ external dependencies"
        ]
        
        if metric in [PerformanceMetric.CPU_USAGE, PerformanceMetric.MEMORY_USAGE]:
            recommendations.append("ØªØ­Ù‚Ù‚ Ù…Ù† running processes Ùˆ memory leaks")
        
        if metric == PerformanceMetric.ERROR_RATE:
            recommendations.append("ÙØ­Øµ application errors ÙˆØ§Ù„Ù€ stack traces")
        
        return recommendations
    
    def predict_future_performance(self) -> Dict[str, Any]:
        """ØªÙˆÙ‚Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ"""
        predictions = {}
        
        for metric, data_points in self.data_buffer.items():
            if len(data_points) < 20:
                continue
            
            try:
                # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                values = [dp.value for dp in data_points[-50:]]  # Ø¢Ø®Ø± 50 Ù†Ù‚Ø·Ø©
                timestamps = [dp.timestamp for dp in data_points[-50:]]
                
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…
                time_numeric = [(t - timestamps[0]).total_seconds() for t in timestamps]
                
                # ØªÙˆÙ‚Ø¹ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
                slope, intercept, r_value, p_value, std_err = stats.linregress(time_numeric, values)
                
                # ØªÙˆÙ‚Ø¹ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
                future_time = time_numeric[-1] + 3600  # Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„
                predicted_value = slope * future_time + intercept
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø©
                confidence = r_value ** 2
                
                # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹
                if abs(slope) < 0.001:
                    trend = "stable"
                elif slope > 0:
                    trend = "increasing"
                else:
                    trend = "decreasing"
                
                predictions[metric.value] = {
                    "predicted_value": predicted_value,
                    "trend": trend,
                    "confidence": confidence,
                    "slope": slope,
                    "current_value": values[-1],
                    "change_percentage": ((predicted_value - values[-1]) / values[-1]) * 100
                }
            
            except Exception as e:
                self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ‚Ø¹ {metric}: {e}")
        
        return predictions
    
    def generate_optimization_recommendations(self) -> List[OptimizationRecommendation]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª ØªØ­Ø³ÙŠÙ† Ø´Ø§Ù…Ù„Ø©"""
        recommendations = []
        
        # ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        current_stats = self.get_current_performance_stats()
        trend_analysis = self.analyze_all_trends()
        predictions = self.predict_future_performance()
        
        # ØªÙˆØµÙŠØ§Øª Ø¹Ù„Ù‰ Ø£Ø³Ø§Ø³ CPU
        cpu_usage = current_stats.get('cpu_usage', {}).get('current', 0)
        if cpu_usage > 70:
            recommendations.append(OptimizationRecommendation(
                id="opt_cpu_001",
                category="performance",
                title="ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU",
                description="Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU Ø£Ø¹Ù„Ù‰ Ù…Ù† 70%ØŒ ÙŠÙˆØµÙ‰ Ø¨ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡",
                impact="high",
                effort="medium",
                expected_improvement=25.0,
                implementation_steps=[
                    "ØªØ­Ù„ÙŠÙ„ bottlenecks ÙÙŠ Ø§Ù„ÙƒÙˆØ¯",
                    "ØªØ­Ø³ÙŠÙ† Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª heavy computation",
                    "Ø§Ø³ØªØ®Ø¯Ø§Ù… async/await Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©",
                    "Ø¥Ø¶Ø§ÙØ© caching Ù„Ù„ØªÙ‚Ù„ÙŠÙ„ Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©"
                ],
                automation_possible=True
            ))
        
        # ØªÙˆØµÙŠØ§Øª Ø¹Ù„Ù‰ Ø£Ø³Ø§Ø³ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        memory_usage = current_stats.get('memory_usage', {}).get('current', 0)
        if memory_usage > 75:
            recommendations.append(OptimizationRecommendation(
                id="opt_memory_001",
                category="performance",
                title="ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©",
                description="Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø£Ø¹Ù„Ù‰ Ù…Ù† 75%ØŒ ÙŠÙˆØµÙ‰ Ø¨ØªØ­Ø³ÙŠÙ† Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©",
                impact="high",
                effort="medium",
                expected_improvement=30.0,
                implementation_steps=[
                    "ØªØ­Ù„ÙŠÙ„ memory leaks",
                    "ØªØ­Ø³ÙŠÙ† data structures",
                    "Ø¥Ø¶Ø§ÙØ© garbage collection tuning",
                    "ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… objects Ø§Ù„Ù…Ø­ØªÙØ¸ Ø¨Ù‡Ø§ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"
                ],
                automation_possible=True
            ))
        
        # ØªÙˆØµÙŠØ§Øª Ø¹Ù„Ù‰ Ø£Ø³Ø§Ø³ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
        response_times = trend_analysis.get('response_time', {})
        if response_times.get('average', 0) > 2.0:
            recommendations.append(OptimizationRecommendation(
                id="opt_response_001",
                category="user_experience",
                title="ØªØ­Ø³ÙŠÙ† Ø²Ù…Ù† Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚",
                description="Ù…ØªÙˆØ³Ø· Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø£Ø¹Ù„Ù‰ Ù…Ù† 2 Ø«Ø§Ù†ÙŠØ©ØŒ ÙŠØ¶Ø± Ø¨ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…",
                impact="high",
                effort="high",
                expected_improvement=40.0,
                implementation_steps=[
                    "ØªØ­Ø³ÙŠÙ† database queries ÙˆØ¥Ø¶Ø§ÙØ© indexes",
                    "Ø§Ø³ØªØ®Ø¯Ø§Ù… connection pooling",
                    "Ø¥Ø¶Ø§ÙØ© Redis caching",
                    "ØªØ­Ø³ÙŠÙ† static assets Ù…Ø¹ CDN",
                    "ØªØ­Ø³ÙŠÙ† frontend loading strategy"
                ],
                automation_possible=False
            ))
        
        # ØªÙˆØµÙŠØ§Øª Ø¹Ù„Ù‰ Ø£Ø³Ø§Ø³ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø®Ø·Ø£
        error_rate = current_stats.get('error_rate', {}).get('current', 0)
        if error_rate > 3:
            recommendations.append(OptimizationRecommendation(
                id="opt_error_001",
                category="reliability",
                title="ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø®Ø·Ø£",
                description=f"Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø®Ø·Ø£ {error_rate}% Ø£Ø¹Ù„Ù‰ Ù…Ù† Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„ØŒ ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚",
                impact="medium",
                effort="high",
                expected_improvement=80.0,
                implementation_steps=[
                    "ØªØ­Ù„ÙŠÙ„ application logs Ù„Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©",
                    "ØªØ­Ø³ÙŠÙ† error handling Ùˆretry logic",
                    "Ø¥Ø¶Ø§ÙØ© monitoring Ù„Ù„external dependencies",
                    "ØªØ­Ø³ÙŠÙ† input validation",
                    "Ø¥Ø¶Ø§ÙØ© fallback mechanisms"
                ],
                automation_possible=False
            ))
        
        # ØªÙˆØµÙŠØ§Øªé¢„æµ‹ÙŠØ©
        if 'cpu_usage' in predictions:
            cpu_prediction = predictions['cpu_usage']
            if cpu_prediction['trend'] == 'increasing' and cpu_prediction['change_percentage'] > 20:
                recommendations.append(OptimizationRecommendation(
                    id="opt_predictive_001",
                    category="scalability",
                    title="Ø§Ø³ØªØ¹Ø¯Ø§Ø¯ Ù„Ù„ØªÙˆØ³Ø¹ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ",
                    description=f"Ù…ØªÙˆÙ‚Ø¹ Ø²ÙŠØ§Ø¯Ø© CPU Ø¨Ù†Ø³Ø¨Ø© {cpu_prediction['change_percentage']:.1f}% ÙÙŠ Ø§Ù„Ø³Ø§Ø¹Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©",
                    impact="high",
                    effort="high",
                    expected_improvement=100.0,
                    implementation_steps=[
                        "Ø¥Ø¹Ø¯Ø§Ø¯ auto-scaling",
                        "ØªØ­Ø³ÙŠÙ† load balancing",
                        "ØªØ­Ø¶ÙŠØ± additional server resources",
                        "ØªØ­Ø³ÙŠÙ† application architecture Ù„Ù„ØªÙˆØ³Ø¹"
                    ],
                    automation_possible=True
                ))
        
        return recommendations
    
    def get_current_performance_stats(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""
        stats = {}
        
        for metric, data_points in self.data_buffer.items():
            if not data_points:
                continue
            
            values = [dp.value for dp in data_points[-100:]]  # Ø¢Ø®Ø± 100 Ù‚ÙŠÙ…Ø©
            
            stats[metric.value] = {
                'current': values[-1] if values else 0,
                'average': np.mean(values) if values else 0,
                'median': np.median(values) if values else 0,
                'std_dev': np.std(values) if values else 0,
                'min': np.min(values) if values else 0,
                'max': np.max(values) if values else 0,
                'percentile_95': np.percentile(values, 95) if values else 0,
                'percentile_99': np.percentile(values, 99) if values else 0
            }
        
        return stats
    
    def analyze_all_trends(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª"""
        trends = {}
        
        for metric in self.data_buffer.keys():
            trend_data = self.analyze_trend(metric)
            
            # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØºÙŠÙŠØ±
            data_points = list(self.data_buffer[metric])
            if len(data_points) >= 10:
                recent_values = [dp.value for dp in data_points[-10:]]
                older_values = [dp.value for dp in data_points[-20:-10]]
                
                if older_values:
                    recent_avg = np.mean(recent_values)
                    older_avg = np.mean(older_values)
                    change_rate = ((recent_avg - older_avg) / older_avg) * 100 if older_avg > 0 else 0
                else:
                    change_rate = 0
            else:
                change_rate = 0
            
            trends[metric.value] = {
                'trend': trend_data,
                'change_rate': change_rate,
                'data_points': len(data_points)
            }
        
        return trends
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ø´Ø§Ù…Ù„"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': {},
            'current_metrics': self.get_current_performance_stats(),
            'trends': self.analyze_all_trends(),
            'predictions': self.predict_future_performance(),
            'anomalies': [asdict(alert) for alert in self.detect_anomalies()],
            'recommendations': [asdict(rec) for rec in self.generate_optimization_recommendations()],
            'health_score': self.calculate_health_score()
        }
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù„Ø®Øµ
        total_metrics = len(report['current_metrics'])
        healthy_metrics = 0
        
        for metric_name, stats in report['current_metrics'].items():
            metric_enum = PerformanceMetric(metric_name)
            if metric_enum in self.alert_thresholds:
                thresholds = self.alert_thresholds[metric_enum]
                current_value = stats['current']
                
                if current_value < thresholds['low']:
                    healthy_metrics += 1
        
        report['summary'] = {
            'total_metrics': total_metrics,
            'healthy_metrics': healthy_metrics,
            'health_percentage': (healthy_metrics / total_metrics * 100) if total_metrics > 0 else 100,
            'critical_issues': len([rec for rec in report['recommendations'] if rec['impact'] == 'high']),
            'needs_attention': len([rec for rec in report['recommendations']])
        }
        
        return report
    
    def calculate_health_score(self) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¹Ø§Ù…Ø©"""
        score = 100.0
        
        for metric_name, stats in self.get_current_performance_stats().items():
            try:
                metric_enum = PerformanceMetric(metric_name)
                current_value = stats['current']
                thresholds = self.alert_thresholds.get(metric_enum, {})
                
                if not thresholds:
                    continue
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®ØµÙ… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø¹Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯
                if current_value >= thresholds['critical']:
                    score -= 30
                elif current_value >= thresholds['high']:
                    score -= 20
                elif current_value >= thresholds['medium']:
                    score -= 10
                elif current_value >= thresholds['low']:
                    score -= 5
            
            except Exception as e:
                self.logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØµØ­Ø© Ù„Ù€ {metric_name}: {e}")
        
        return max(0.0, score)
    
    def export_metrics_prometheus(self) -> str:
        """ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¨ØµÙŠØºØ© Prometheus"""
        metrics_text = ""
        timestamp = datetime.now().timestamp()
        
        for metric_name, stats in self.get_current_performance_stats().items():
            if stats['current'] is not None:
                metrics_text += f"{metric_name}_current {stats['current']} {timestamp}\n"
                metrics_text += f"{metric_name}_average {stats['average']} {timestamp}\n"
                metrics_text += f"{metric_name}_percentile_95 {stats['percentile_95']} {timestamp}\n"
        
        return metrics_text
    
    def visualize_performance_data(self, save_path: str = None):
        """ØªØµÙˆØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        if not save_path:
            save_path = f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data = {}
        for metric, data_points in self.data_buffer.items():
            if len(data_points) >= 10:
                data[metric.value] = {
                    'timestamps': [dp.timestamp for dp in data_points[-100:]],
                    'values': [dp.value for dp in data_points[-100:]]
                }
        
        if not data:
            print("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØµÙˆØ±")
            return
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø®Ø·Ø·Ø§Øª
        fig, axes = plt.subplots(len(data), 1, figsize=(12, 4 * len(data)))
        if len(data) == 1:
            axes = [axes]
        
        for i, (metric_name, metric_data) in enumerate(data.items()):
            ax = axes[i]
            timestamps = metric_data['timestamps']
            values = metric_data['values']
            
            ax.plot(timestamps, values, marker='o', markersize=3, alpha=0.7)
            ax.set_title(f'{metric_name} - Ø¢Ø®Ø± 100 Ù†Ù‚Ø·Ø©')
            ax.set_ylabel('Ø§Ù„Ù‚ÙŠÙ…Ø©')
            ax.grid(True, alpha=0.3)
            
            # ØªØ¯ÙˆÙŠØ± Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ù„Ù„Ø£ÙØ¶Ù„
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¨ØµØ±ÙŠ ÙÙŠ: {save_path}")
    
    async def start_monitoring(self):
        """Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©"""
        self.logger.info("Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡...")
        
        while True:
            try:
                # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                data_points = self.collect_performance_data()
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                for data_point in data_points:
                    self.add_data_point(data_point)
                
                # Ø§Ù†ØªØ¸Ø§Ø± ÙØªØ±Ø© Ø§Ù„ØªØ¬Ù…ÙŠØ¹
                await asyncio.sleep(self.analysis_interval)
                
            except KeyboardInterrupt:
                self.logger.info("ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡")
                break
            except Exception as e:
                self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
                await asyncio.sleep(10)  # Ø§Ù†ØªØ¸Ø§Ø± 10 Ø«ÙˆØ§Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰


class AdvancedPerformanceMonitor:
    """Ù…Ø±Ø§Ù‚Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    
    def __init__(self, config_path: str = None):
        self.config = self.load_config(config_path)
        self.analyzer = PerformanceAnalyzer(self.config.get('analyzer', {}))
        self.data_collector = DataCollector(self.config.get('collector', {}))
        self.alert_manager = AlertManager(self.config.get('alerts', {}))
        self.optimization_engine = OptimizationEngine(self.config.get('optimization', {}))
        
        self.logger = self.setup_logging()
        
    def load_config(self, config_path: str) -> Dict[str, Any]:
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª"""
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        return self.get_default_config()
    
    def get_default_config(self) -> Dict[str, Any]:
        """Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©"""
        return {
            "analyzer": {
                "analysis_interval": 30,
                "prediction_window": 3600
            },
            "collector": {
                "data_sources": {
                    "application": {
                        "enabled": True,
                        "endpoints": ["/health", "/metrics", "/status"]
                    },
                    "system": {
                        "enabled": True,
                        "metrics": ["cpu", "memory", "disk", "network"]
                    },
                    "database": {
                        "enabled": True,
                        "connections": ["postgresql://localhost:5432"]
                    },
                    "cache": {
                        "enabled": True,
                        "redis_url": "redis://localhost:6379"
                    }
                }
            },
            "alerts": {
                "enabled": True,
                "channels": ["email", "slack"],
                "thresholds": {
                    "response_time": 5.0,
                    "error_rate": 5.0,
                    "cpu_usage": 80.0,
                    "memory_usage": 85.0
                }
            },
            "optimization": {
                "auto_optimization": False,
                "recommendations_enabled": True,
                "machine_learning": {
                    "anomaly_detection": True,
                    "trend_prediction": True
                }
            }
        }
    
    def setup_logging(self) -> logging.Logger:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ø³Ø¬Ù„Ø§Øª"""
        logger = logging.getLogger('AdvancedPerformanceMonitor')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    async def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """ØªØ´ØºÙŠÙ„ ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø£Ø¯Ø§Ø¡"""
        self.logger.info("Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ø£Ø¯Ø§Ø¡...")
        
        # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data_points = await self.data_collector.collect_all_metrics()
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù…Ø­Ù„Ù„
        for data_point in data_points:
            self.analyzer.add_data_point(data_point)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
        report = self.analyzer.generate_performance_report()
        
        # ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ°
        anomalies = self.analyzer.detect_anomalies()
        report['detected_anomalies'] = anomalies
        
        # Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª
        predictions = self.analyzer.predict_future_performance()
        report['performance_predictions'] = predictions
        
        # Ø§Ù„ØªÙˆØµÙŠØ§Øª
        recommendations = self.analyzer.generate_optimization_recommendations()
        report['optimization_recommendations'] = recommendations
        
        # Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙØ¹Ù„Ø§Ù‹)
        if self.config.get('optimization', {}).get('auto_optimization'):
            await self.apply_automatic_optimizations(recommendations)
        
        self.logger.info(f"Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ - Ø¯Ø±Ø¬Ø© Ø§Ù„ØµØ­Ø©: {report['health_score']:.1f}")
        
        return report
    
    async def apply_automatic_optimizations(self, recommendations: List[OptimizationRecommendation]):
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©"""
        self.logger.info("Ø¨Ø¯Ø¡ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©...")
        
        for recommendation in recommendations:
            if not recommendation.automation_possible:
                continue
            
            try:
                self.logger.info(f"ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†: {recommendation.title}")
                
                # ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙˆØµÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø©
                if recommendation.category == "performance":
                    await self.apply_performance_optimization(recommendation)
                elif recommendation.category == "scalability":
                    await self.apply_scaling_optimization(recommendation)
                elif recommendation.category == "reliability":
                    await self.apply_reliability_optimization(recommendation)
                
                self.logger.info(f"ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆØµÙŠØ©: {recommendation.title}")
                
            except Exception as e:
                self.logger.error(f"ÙØ´Ù„ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆØµÙŠØ© {recommendation.title}: {e}")
    
    async def apply_performance_optimization(self, recommendation: OptimizationRecommendation):
        """ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        if recommendation.id == "opt_cpu_001":
            # ØªØ­Ø³ÙŠÙ† CPU
            await self.optimize_cpu_usage()
        elif recommendation.id == "opt_memory_001":
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            await self.optimize_memory_usage()
    
    async def apply_scaling_optimization(self, recommendation: OptimizationRecommendation):
        """ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ØªÙˆØ³Ø¹"""
        if recommendation.id == "opt_predictive_001":
            # ØªØ­Ø¶ÙŠØ± Ù„Ù„ØªÙˆØ³Ø¹
            await self.prepare_for_scaling()
    
    async def apply_reliability_optimization(self, recommendation: OptimizationRecommendation):
        """ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©"""
        if recommendation.id == "opt_error_001":
            # ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø®Ø·Ø£
            await self.improve_error_handling()
    
    async def optimize_cpu_usage(self):
        """ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU"""
        # ØªÙ†ÙÙŠØ° ØªØ­Ø³ÙŠÙ†Ø§Øª CPU
        # Ù…Ø«Ù„ ØªØ­Ø³ÙŠÙ† garbage collectionØŒ ØªØ­Ø³ÙŠÙ† threadsØŒ etc.
        pass
    
    async def optimize_memory_usage(self):
        """ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        # ØªÙ†ÙÙŠØ° ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        pass
    
    async def prepare_for_scaling(self):
        """ØªØ­Ø¶ÙŠØ± Ù„Ù„ØªÙˆØ³Ø¹"""
        # ØªÙ†ÙÙŠØ° Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ØªØ­Ø¶ÙŠØ± Ù„Ù„ØªÙˆØ³Ø¹
        pass
    
    async def improve_error_handling(self):
        """ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
        # ØªÙ†ÙÙŠØ° ØªØ­Ø³ÙŠÙ†Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        pass
    
    def start_monitoring_dashboard(self):
        """Ø¨Ø¯Ø¡ Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©"""
        self.logger.info("Ø¨Ø¯Ø¡ Ù„ÙˆØ­Ø© Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡...")
        
        # ÙŠÙ…ÙƒÙ† ØªÙ†ÙÙŠØ° dashboard Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Streamlit Ø£Ùˆ React
        # Ø£Ùˆ ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù€ Grafana
        
        try:
            import streamlit as st
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ ØµÙØ­Ø© Streamlit
            st.set_page_config(page_title="Ù…Ø±Ø§Ù‚Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…", layout="wide")
            st.title("ğŸ“Š Ù…Ø±Ø§Ù‚Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
            
            # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ø§Ù…Ø©
            col1, col2, col3, col4 = st.columns(4)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            report = self.analyzer.generate_performance_report()
            
            # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
            with col1:
                st.metric("Ø¯Ø±Ø¬Ø© Ø§Ù„ØµØ­Ø©", f"{report['health_score']:.1f}%")
            
            with col2:
                st.metric("Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU", f"{report['current_metrics'].get('cpu_usage', {}).get('current', 0):.1f}%")
            
            with col3:
                st.metric("Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©", f"{report['current_metrics'].get('memory_usage', {}).get('current', 0):.1f}%")
            
            with col4:
                st.metric("Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©", f"{report['current_metrics'].get('response_time', {}).get('current', 0):.2f}s")
            
            # Ø§Ù„ØªÙˆØµÙŠØ§Øª
            st.subheader("ğŸ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª")
            for rec in report['optimization_recommendations'][:5]:
                with st.expander(f"{rec['title']} (ØªØ£Ø«ÙŠØ±: {rec['impact']})"):
                    st.write(rec['description'])
                    st.write(f"Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {rec['expected_improvement']:.1f}%")
            
            # Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ù…ÙƒØªØ´Ù
            if report['anomalies']:
                st.subheader("âš ï¸ Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ù…ÙƒØªØ´Ù")
                for anomaly in report['anomalies']:
                    st.warning(f"{anomaly['metric']}: {anomaly['message']}")
            
            # Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡ Ù„Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
            if st.button("ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"):
                st.rerun()
            
            st.info("Ø³ÙŠØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ ÙƒÙ„ 30 Ø«Ø§Ù†ÙŠØ©")
            
        except ImportError:
            st.warning("Streamlit ØºÙŠØ± Ù…ØªÙˆÙØ±. Ø³ÙŠØªÙ… ØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„Ùƒ.")
            self.export_report_to_json()
    
    def export_report_to_json(self, filepath: str = None):
        """ØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¥Ù„Ù‰ JSON"""
        if not filepath:
            filepath = f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        report = self.analyzer.generate_performance_report()
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ØªÙ… ØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¥Ù„Ù‰: {filepath}")


class DataCollector:
    """Ø¬Ø§Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger('DataCollector')
    
    async def collect_all_metrics(self) -> List[PerformanceDataPoint]:
        """Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³"""
        data_points = []
        
        # Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ø¸Ø§Ù…
        if self.config.get('data_sources', {}).get('system', {}).get('enabled'):
            system_metrics = await self.collect_system_metrics()
            data_points.extend(system_metrics)
        
        # Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
        if self.config.get('data_sources', {}).get('application', {}).get('enabled'):
            app_metrics = await self.collect_application_metrics()
            data_points.extend(app_metrics)
        
        # Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if self.config.get('data_sources', {}).get('database', {}).get('enabled'):
            db_metrics = await self.collect_database_metrics()
            data_points.extend(db_metrics)
        
        # Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        if self.config.get('data_sources', {}).get('cache', {}).get('enabled'):
            cache_metrics = await self.collect_cache_metrics()
            data_points.extend(cache_metrics)
        
        return data_points
    
    async def collect_system_metrics(self) -> List[PerformanceDataPoint]:
        """Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ø¸Ø§Ù…"""
        data_points = []
        timestamp = datetime.now()
        
        try:
            # CPU
            cpu_percent = psutil.cpu_percent(interval=1)
            data_points.append(PerformanceDataPoint(
                timestamp=timestamp,
                metric=PerformanceMetric.CPU_USAGE,
                value=cpu_percent,
                unit="percent",
                labels={}
            ))
            
            # Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            memory = psutil.virtual_memory()
            data_points.append(PerformanceDataPoint(
                timestamp=timestamp,
                metric=PerformanceMetric.MEMORY_USAGE,
                value=memory.percent,
                unit="percent",
                labels={}
            ))
            
            # Ø§Ù„Ù‚Ø±Øµ
            disk_io = psutil.disk_io_counters()
            if disk_io:
                data_points.append(PerformanceDataPoint(
                    timestamp=timestamp,
                    metric=PerformanceMetric.DISK_IO,
                    value=disk_io.read_bytes + disk_io.write_bytes,
                    unit="bytes",
                    labels={}
                ))
            
            # Ø§Ù„Ø´Ø¨ÙƒØ©
            network_io = psutil.net_io_counters()
            if network_io:
                data_points.append(PerformanceDataPoint(
                    timestamp=timestamp,
                    metric=PerformanceMetric.NETWORK_IO,
                    value=network_io.bytes_sent + network_io.bytes_recv,
                    unit="bytes",
                    labels={}
                ))
        
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ø¸Ø§Ù…: {e}")
        
        return data_points
    
    async def collect_application_metrics(self) -> List[PerformanceDataPoint]:
        """Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚"""
        # ÙŠÙ…ÙƒÙ† ØªÙ†ÙÙŠØ° Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù…Ù† endpoints Ù…Ø®ØªÙ„ÙØ©
        return []
    
    async def collect_database_metrics(self) -> List[PerformanceDataPoint]:
        """Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        # ÙŠÙ…ÙƒÙ† ØªÙ†ÙÙŠØ° Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„
        return []
    
    async def collect_cache_metrics(self) -> List[PerformanceDataPoint]:
        """Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©"""
        # ÙŠÙ…ÙƒÙ† ØªÙ†ÙÙŠØ° Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Redis Ø£Ùˆ Memcached
        return []


class AlertManager:
    """Ù…Ø¯ÙŠØ± Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger('AlertManager')
    
    def send_alert(self, alert: PerformanceAlert):
        """Ø¥Ø±Ø³Ø§Ù„ ØªÙ†Ø¨ÙŠÙ‡"""
        self.logger.warning(f"ØªÙ†Ø¨ÙŠÙ‡: {alert.message}")
        # ÙŠÙ…ÙƒÙ† Ø±Ø¨Ø·Ù‡ Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯


class OptimizationEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ø³ÙŠÙ†"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger('OptimizationEngine')
    
    async def apply_optimization(self, recommendation: OptimizationRecommendation):
        """ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†"""
        self.logger.info(f"ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†: {recommendation.title}")
        # ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ø³ÙŠÙ†


# Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…
async def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("ğŸš€ Ø¨Ø¯Ø¡ Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…...")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø±Ø§Ù‚Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡
    monitor = AdvancedPerformanceMonitor()
    
    try:
        # ØªØ´ØºÙŠÙ„ ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„
        print("\nğŸ“Š ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„...")
        report = await monitor.run_comprehensive_analysis()
        
        print(f"\n=== ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡ ===")
        print(f"Ø§Ù„ÙˆÙ‚Øª: {report['timestamp']}")
        print(f"Ø¯Ø±Ø¬Ø© Ø§Ù„ØµØ­Ø©: {report['health_score']:.1f}%")
        print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³: {report['summary']['total_metrics']}")
        print(f"Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØµØ­ÙŠØ©: {report['summary']['healthy_metrics']}")
        print(f"Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: {report['summary']['needs_attention']}")
        
        # Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØµÙŠØ§Øª
        if report['optimization_recommendations']:
            print(f"\nğŸ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª ({len(report['optimization_recommendations'])}):")
            for rec in report['optimization_recommendations'][:3]:
                print(f"- {rec['title']} (ØªØ£Ø«ÙŠØ±: {rec['impact']}, ØªØ­Ø³ÙŠÙ† Ù…ØªÙˆÙ‚Ø¹: {rec['expected_improvement']:.1f}%)")
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        monitor.export_report_to_json()
        
        # Ø¥Ù†Ø´Ø§Ø¡ ØªØµÙˆØ±
        monitor.analyzer.visualize_performance_data()
        
        # Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© dashboard
        print(f"\nğŸŒ Ø¨Ø¯Ø¡ dashboard Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©...")
        print("Ø§Ø¶ØºØ· Ctrl+C Ù„Ù„Ø¥ÙŠÙ‚Ø§Ù")
        
        # ØªØ´ØºÙŠÙ„ dashboard
        monitor.start_monitoring_dashboard()
        
    except KeyboardInterrupt:
        print(f"\nâœ… ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…
    asyncio.run(main())
